{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d700f771",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the TinyStories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Print dataset information\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfeb112-e480-41f2-82e0-22c8cf1335f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (5%): 105985\n",
      "Validation size (5%): 1099\n",
      "Test size (5%): 1099\n",
      "\n",
      "=== First 5 validation samples ===\n",
      "1: {'text': 'Once upon a time, there was a princess who had an awful problem. She hated her weight. Everywhere she went she felt so sorry and sad.\\n\\nThe princess wanted so badly to be thin like the other animals in her kingdom. She tried to do everything to make her weight go away - she ate less and ran and jumped, but nothing worked.\\n\\nOne day, something magical happened. A magical fairy appeared and waved her wand and said, \"No more weight for the princess\". And, just like that, the princess was free from her weight.\\n\\nThe princess was so, so happy. She thanked the fairy again and again and ran off dancing and singing.\\n\\nThe princess never worried about her weight again, she was so happy, and thanked the magical fairy every day.'}\n",
      "2: {'text': \"Once upon a time, there was a little boy named Timmy. Timmy was very hungry and wanted to eat a cookie. But his mom said he had to eat his dinner first. Timmy didn't like his dinner, but he knew he had to eat it to be healthy.\\n\\nAfter dinner, Timmy's dad took him to the barber. The barber cut Timmy's hair and made him look very handsome. Timmy liked his new haircut and felt very happy.\\n\\nOn the way home, they saw a homeless man who was also very hungry. Timmy's dad gave the man some food and Timmy watched as the man ate it all up. Timmy felt happy that he helped someone who was hungry.\\n\\nThe moral of the story is that it's important to eat your dinner even if you're hungry for something else. And it's also important to help others who are hungry too.\"}\n",
      "3: {'text': \"John dropped his toy train on the ground while he was playing. He wanted to get it back, but it had landed in a small hole in the ground. It was so small that he couldn't fit his hand in. His mom told him it was an underground. She said it was too expensive to get back.\\n\\nJohn was very sad and started to cry. He wanted to find a way to get his toy back. Then, he had an idea. He asked for a large bucket from the kitchen and took it outside. He put the bucket over the underground and started to fill it up with water from the garden hose.\\n\\nLittle by little, the water began to fill up the underground and the toy train started to float. John was so happy and grabbed it before it could drop again.\\n\\nHis mom was proud of John for his clever thinking and said that sometimes being expensive is not the best way to solve a problem.\"}\n",
      "4: {'text': \"Once upon a time, there was a little mouse named Max. Max loved to eat cheese, but he could never find the perfect cheese. One day, Max met a wise old owl who told him that he could make his own perfect cheese.\\n\\nMax was very excited and asked the owl how to make perfect cheese. The owl told Max that he needed to find the best milk and mix it with a special ingredient. Max thanked the owl and went on his way to find the ingredients.\\n\\nAfter searching for a long time, Max finally found the best milk and mixed it with the special ingredient. He made the perfect cheese! Max was so happy and proud of himself.\\n\\nThe moral of the story is that if you want something perfect, you have to make it yourself. Max learned that he didn't have to rely on others to find what he was looking for. He could make it himself with a little help from a wise friend.\"}\n",
      "5: {'text': 'Once upon a time, there was a little girl named Lily. She loved to play with her toys, especially her spinning top. One day, she was playing with her top when she heard a loud noise outside. She looked out the window and saw a man playing a trumpet.\\n\\nLily was nervous because she had never seen a trumpet before. She asked her mom, \"What\\'s that noise?\" Her mom replied, \"That\\'s a man playing a trumpet. It makes music.\"\\n\\nLily was curious, so she went outside to listen. The man saw her and said, \"Do you want to try playing my trumpet?\" Lily was excited and said yes. She took the trumpet and blew into it, but nothing happened. The man said, \"You have to spin the mouthpiece to make it work.\" Lily spun the mouthpiece and suddenly music came out. She was so happy and danced along to the music. From that day on, Lily loved to spin the trumpet\\'s mouthpiece and make music.'}\n",
      "\n",
      "=== First 5 test samples ===\n",
      "1: {'text': \"Once upon a time, there was a little girl who loved to explore the nature. She wore her favorite red boots and ventured out. As the little girl stepped into the nature, she noticed how thick the grass felt beneath her feet.\\n\\nThe little girl stepped deeper into the tall grass, feeling the warmth of the sun's rays on her skin. She was in awe of the beauty of her surroundings. The sun shone high in the sky, and the air was filled with the sweet scent of wildflowers.\\n\\nThe little girl stepped closer to a beautiful bush, the leaves the most vibrant green and the petals of the wildflowers a beautiful sight. She reached out her small hand, her fingertips touching the delicate petals.\\n\\nThe little girl stepped ever closer to the nature, feeling the thick ground beneath her feet. Everywhere she looked, she could see beauty. She could hear the birds singing around her and the gentle breeze blowing around her.\\n\\nThe little girl stepped onto the path home and said goodbye to the beautiful world of nature. But she knew that she would be back again soon to explore the beauty of the nature.\"}\n",
      "2: {'text': 'Once upon a time, there was a generous rabbit that lived in a castle. He loved to chew on delicious carrots, and he always made sure to leave some for his friends in the castle.\\n\\nEvery day the rabbit would chew on a big, juicy carrot, and then take little nibbles for the other animals in the castle. He was so generous with his carrots, that he became known as the Carrot King!\\n\\nAll of the other animals in the castle were very happy to have the Carrot King around. He was a kind and gentle rabbit, always willing to share and help others.\\n\\nWhen the other animals needed help, they would come to the Carrot King and ask for carrots. He was always happy to share, and would give them as many carrots as they wanted.\\n\\nThe Carrot King was loved and respected by all the animals in the castle. He was the king of generosity!'}\n",
      "3: {'text': 'Once upon a time, in a bright sunny day, a little dog went out to play. He liked to run and jump in the park. The little dog was very happy.\\n\\nWhile playing, the little dog found a red ball. He did not know who the ball belonged to. He wanted to find the owner of the ball. The little dog picked up the ball and started to look for the owner.\\n\\nThe little dog showed the red ball to many people in the park. At last, he found a little girl who was looking for her ball. She was the owner! The little girl was very happy and thanked the little dog. They played with the red ball together and had a lot of fun.'}\n",
      "4: {'text': 'Once upon a time, there was a little bird named Blue. Blue liked to fly high up in the sky and look at the pretty clouds. One day, Blue saw a big rock on the ground and decided to land on it.\\n\\nSuddenly, a nosy rabbit hopped over and asked, \"What are you doing on that rock?\"\\n\\n\"I\\'m just resting,\" replied Blue.\\n\\nThe rabbit said, \"I wish I could fly like you. It must be so much fun!\"\\n\\n\"It is,\" said Blue. \"Do you want to come with me?\"\\n\\nThe rabbit hopped onto Blue\\'s back and they flew together through the sky. The rabbit was so happy to fly and see the world from up high. From then on, Blue and the rabbit were the best of friends and flew together every day.'}\n",
      "5: {'text': \"Tim was a big boy who liked to march. He marched in his room, he marched in the hall, he marched in the yard. He marched with his feet, he marched with his hands, he marched with his toys. He liked to make loud noises when he marched, like boom, boom, boom.\\n\\nOne day, Tim saw a large crib in his mom's room. He wondered what was inside. He marched to the crib and looked over the edge. He saw a small baby sleeping. He wanted to march with the baby. He climbed into the crib and started to march. He marched on the blanket, he marched on the pillow, he marched on the baby. He made loud noises, like boom, boom, boom.\\n\\nThe baby woke up and started to cry. Tim did not like the cry. He wanted the baby to march with him. He tried to make the baby march, but the baby did not want to. The baby cried louder and louder. Tim got angry and pushed the baby. The baby fell out of the crib and hit the floor. The baby did not move.\\n\\nTim's mom heard the cry and the thud. She ran to the room and saw the baby on the floor. She screamed and picked up the baby. She saw blood on the baby's head. She cried and called for help. She did not look at Tim.\\n\\nTim did not understand. He wanted to march with the baby. He did not mean to hurt the baby. He felt sad and scared. He climbed out of the crib and marched to his room. He marched with his feet, he marched with his hands, he marched with his toys. He made loud noises, like boom, boom, boom. But he did not feel happy. He felt alone.\"}\n"
     ]
    }
   ],
   "source": [
    "# Function to sample 5% of the dataset with optional seed and offset\n",
    "def sample_five_percent(dataset_split, seed=42, offset=0):\n",
    "    total_size = len(dataset_split)\n",
    "    five_percent_size = total_size // 20  # 5% of the dataset\n",
    "    shuffled = dataset_split.shuffle(seed=seed)\n",
    "    return shuffled.select(range(offset, offset + five_percent_size))\n",
    "\n",
    "# Sample 5% from train, validation, and test splits\n",
    "train_data = sample_five_percent(dataset['train'], seed=42)\n",
    "val_data   = sample_five_percent(dataset['validation'], seed=123, offset=0)\n",
    "test_data  = sample_five_percent(dataset['validation'], seed=123, offset=len(val_data))  # next 5%\n",
    "\n",
    "# Check sizes\n",
    "print(f\"Train size (5%): {len(train_data)}\")\n",
    "print(f\"Validation size (5%): {len(val_data)}\")\n",
    "print(f\"Test size (5%): {len(test_data)}\")\n",
    "\n",
    "# Show first five samples from val_data\n",
    "print(\"\\n=== First 5 validation samples ===\")\n",
    "for i in range(5):\n",
    "    print(f\"{i + 1}: {val_data[i]}\")\n",
    "\n",
    "# Show first five samples from test_data\n",
    "print(\"\\n=== First 5 test samples ===\")\n",
    "for i in range(5):\n",
    "    print(f\"{i + 1}: {test_data[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddde6a3-fbe7-40a7-bba8-fbd28a33f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to EOS token to avoid warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d084e439-dc4b-47bb-b40f-a71e6210201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [14967, 290, 32189, 588, 284, 711, 287, 262, 3952, 13, 1119, 766, 257, 1263, 3430, 319, 262, 2323, 13, 632, 318, 7586, 290, 890, 290, 4334, 13, 198, 198, 1, 8567, 11, 257, 3430, 2474, 5045, 1139, 13, 366, 40, 460, 10303, 340, 2474, 198, 198, 1544, 8404, 284, 10303, 262, 3430, 11, 475, 340, 318, 1165, 5802, 13, 679, 8953, 866, 290, 10532, 262, 3430, 13, 198, 198, 1, 46, 794, 2474, 339, 1139, 13, 366, 2504, 5938, 2474, 198, 198, 44, 544, 22051, 13, 1375, 318, 407, 1612, 11, 673, 655, 6834, 340, 318, 8258, 13, 198, 198, 1, 5756, 502, 1949, 2474, 673, 1139, 13, 366, 40, 460, 5236, 340, 2474, 198, 198, 3347, 11103, 510, 262, 3430, 290, 7584, 340, 319, 607, 1182, 13, 1375, 11114, 6364, 290, 7773, 13, 1375, 857, 407, 2121, 866, 13, 198, 198, 1, 22017, 2474, 5045, 1139, 13, 366, 1639, 389, 922, 379, 22486, 2474, 198, 198, 1, 10449, 345, 2474, 32189, 1139, 13, 366, 1026, 318, 1257, 2474, 198, 198, 2990, 1011, 4962, 22486, 262, 3430, 319, 511, 6665, 11, 5101, 11, 290, 7405, 13, 1119, 423, 257, 1256, 286, 1257, 351, 262, 3430, 13, 1119, 389, 3772, 290, 6613, 13, 1119, 389, 922, 2460, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [14967, 290, 32189, 588, 284, 711, 287, 262, 3952, 13, 1119, 766, 257, 1263, 3430, 319, 262, 2323, 13, 632, 318, 7586, 290, 890, 290, 4334, 13, 198, 198, 1, 8567, 11, 257, 3430, 2474, 5045, 1139, 13, 366, 40, 460, 10303, 340, 2474, 198, 198, 1544, 8404, 284, 10303, 262, 3430, 11, 475, 340, 318, 1165, 5802, 13, 679, 8953, 866, 290, 10532, 262, 3430, 13, 198, 198, 1, 46, 794, 2474, 339, 1139, 13, 366, 2504, 5938, 2474, 198, 198, 44, 544, 22051, 13, 1375, 318, 407, 1612, 11, 673, 655, 6834, 340, 318, 8258, 13, 198, 198, 1, 5756, 502, 1949, 2474, 673, 1139, 13, 366, 40, 460, 5236, 340, 2474, 198, 198, 3347, 11103, 510, 262, 3430, 290, 7584, 340, 319, 607, 1182, 13, 1375, 11114, 6364, 290, 7773, 13, 1375, 857, 407, 2121, 866, 13, 198, 198, 1, 22017, 2474, 5045, 1139, 13, 366, 1639, 389, 922, 379, 22486, 2474, 198, 198, 1, 10449, 345, 2474, 32189, 1139, 13, 366, 1026, 318, 1257, 2474, 198, 198, 2990, 1011, 4962, 22486, 262, 3430, 319, 511, 6665, 11, 5101, 11, 290, 7405, 13, 1119, 423, 257, 1256, 286, 1257, 351, 262, 3430, 13, 1119, 389, 3772, 290, 6613, 13, 1119, 389, 922, 2460, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]}\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and prepare inputs/labels\n",
    "def tokenize_function_with_labels(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Pad to a fixed length for batches\n",
    "        max_length=512         # Set maximum length for sequences\n",
    "    )\n",
    "    # Add labels (same as input_ids for language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize train and validation datasets\n",
    "train_dataset = train_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = val_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Verify tokenized data structure\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64cdd21d-3e44-48f5-ba89-9522cc9b04b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 model with a language modeling head\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resize embeddings to account for padding token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eca2bc4-9f8c-481b-92c2-1a1288194f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-tinystories3\",  # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"steps\",     # Evaluate at specific intervals\n",
    "    eval_steps=500,                  # Evaluate every 500 steps\n",
    "    learning_rate=5e-5,              # Learning rate for optimizer\n",
    "    weight_decay=0.01,               # Regularization strength\n",
    "    per_device_train_batch_size=4,   # Training batch size\n",
    "    per_device_eval_batch_size=4,    # Evaluation batch size\n",
    "    num_train_epochs=3,              # Total number of training epochs\n",
    "    save_strategy=\"steps\",           # Save model at specific intervals\n",
    "    save_steps=500,                  # Save model every 500 steps\n",
    "    logging_dir=\"./logs\",            # Directory for training logs\n",
    "    save_total_limit=3,              # Limit total number of saved checkpoints\n",
    "    load_best_model_at_end=True,     # Load best model after training\n",
    "    save_safetensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f75f4cca-a633-4299-8acd-9ba957f2a081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.0)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\program files\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.3/1.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 4.5 MB/s eta 0:00:00\n",
      "Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy, tf-keras\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-2.1.3 tf-keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~.mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~.mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ultralytics 8.3.77 requires numpy<=2.1.1,>=1.23.0, but you have numpy 2.1.3 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires keras<2.14,>=2.13.1, but you have keras 3.9.0 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires numpy<=1.24.3,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.19.0 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.12.0 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2828df99-6948-4d4a-a020-ad4b9d6f743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Add EarlyStoppingCallback to stop training if validation loss does not improve\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c315e30-9627-4d73-b31a-7402584fed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksedr\\AppData\\Local\\Temp\\ipykernel_33080\\1833781753.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b78c0bac-f596-48e9-9702-46e689933e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79491' max='79491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79491/79491 10:28:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.768094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.799800</td>\n",
       "      <td>0.740806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.769700</td>\n",
       "      <td>0.723584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.764600</td>\n",
       "      <td>0.709567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.699948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.733100</td>\n",
       "      <td>0.692197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.732400</td>\n",
       "      <td>0.686280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>0.680139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.722500</td>\n",
       "      <td>0.675569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.719900</td>\n",
       "      <td>0.671114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.669680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.664833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>0.660512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.702600</td>\n",
       "      <td>0.657850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.656783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.702700</td>\n",
       "      <td>0.653905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.652447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.648981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.646410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>0.644795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.671100</td>\n",
       "      <td>0.643523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.686400</td>\n",
       "      <td>0.641895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.682600</td>\n",
       "      <td>0.639573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.638423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.636306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.665900</td>\n",
       "      <td>0.634262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.633011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.631394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.672100</td>\n",
       "      <td>0.631215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.628419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.628019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.661100</td>\n",
       "      <td>0.625852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.626308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.624346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.622607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.620920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.620330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>0.618826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.647700</td>\n",
       "      <td>0.618358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.655500</td>\n",
       "      <td>0.617386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.617524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.616563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.614317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.613735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.613822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.649400</td>\n",
       "      <td>0.611121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.639800</td>\n",
       "      <td>0.610604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.609851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>0.609541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.645600</td>\n",
       "      <td>0.608358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.640900</td>\n",
       "      <td>0.607622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>0.606910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.643400</td>\n",
       "      <td>0.606495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.629300</td>\n",
       "      <td>0.606648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.623600</td>\n",
       "      <td>0.605353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.632000</td>\n",
       "      <td>0.603889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.620300</td>\n",
       "      <td>0.604579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.603409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.620700</td>\n",
       "      <td>0.602374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>0.601410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>0.601826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.618800</td>\n",
       "      <td>0.600342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.625800</td>\n",
       "      <td>0.600316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.599230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.614400</td>\n",
       "      <td>0.598965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.598602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.598840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.597670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.597148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.596513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.595798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.595743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.613800</td>\n",
       "      <td>0.594817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.594913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.593945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.593952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.614200</td>\n",
       "      <td>0.592980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.592778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.609700</td>\n",
       "      <td>0.592170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.591723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>0.590870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>0.591345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.590511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.589967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.612200</td>\n",
       "      <td>0.590507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.606000</td>\n",
       "      <td>0.589510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.588303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.587704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.588110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.587741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.587395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.616400</td>\n",
       "      <td>0.586658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.614900</td>\n",
       "      <td>0.586285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.586220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.610500</td>\n",
       "      <td>0.585572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.614900</td>\n",
       "      <td>0.584607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.609500</td>\n",
       "      <td>0.584341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>0.583961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.583850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.583852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.583212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>0.582229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.581900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.582241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>0.581510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>0.581757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.581181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.581361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.580485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.580934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>0.580306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.586900</td>\n",
       "      <td>0.580007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.586100</td>\n",
       "      <td>0.580173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.580091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.579846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.579302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.594300</td>\n",
       "      <td>0.579270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.578783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>0.578698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.583900</td>\n",
       "      <td>0.578529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.590300</td>\n",
       "      <td>0.577998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.578525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.577578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.577460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.577454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.575700</td>\n",
       "      <td>0.577345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.588700</td>\n",
       "      <td>0.576645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>0.576657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>0.576172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>0.576003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.590500</td>\n",
       "      <td>0.576130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.591300</td>\n",
       "      <td>0.576057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.575753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.589000</td>\n",
       "      <td>0.575552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.575338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.583100</td>\n",
       "      <td>0.574815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>0.574840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.574676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.574531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.574584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.579200</td>\n",
       "      <td>0.574326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>0.574036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.588300</td>\n",
       "      <td>0.573975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.573780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>0.573548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.579300</td>\n",
       "      <td>0.573404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.586900</td>\n",
       "      <td>0.573193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.573325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.575800</td>\n",
       "      <td>0.572971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.576700</td>\n",
       "      <td>0.572818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>0.572790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>0.572850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.572626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.579900</td>\n",
       "      <td>0.572607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>0.572549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.572463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.572472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=79491, training_loss=0.6278838022936559, metrics={'train_runtime': 37723.3402, 'train_samples_per_second': 8.429, 'train_steps_per_second': 2.107, 'total_flos': 8.307910803456e+16, 'train_loss': 0.6278838022936559, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3000f26-03ec-499a-aa89-b005aff31912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "Once upon a time in a magical forest, there lived a brave knight. Everyday he would wear his special helmet. Everyday he would set off on an adventure. He would explore the forest and discover new things.\n",
      "\n",
      "One day the knight heard a strange sound. It was thundering and it made him very frightened. He was about to go home but his helmet saved him! He slowly put it on and before he knew it, he was safe.\n",
      "\n",
      "The knight decided that it was time to set and to go home without any worries. He ran back and forth but nothing happened but he knew that he had been brave. \n",
      "\n",
      "Finally, when the knight was safe by a tree, he thanked God and promised to always keep his\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"Once upon a time in a magical forest\"\n",
    "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11c4e6e-1890-4c9a-98f6-746574d6ef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "One day I was walking in an ancient desert when dark clouds started to pour down. It was pouring rain, and it was getting cold. Suddenly a thunderstorm came and it rained. It's power was out and the field was so empty. But then a tall figure came out of the clouds. It was a giant elephant!\n",
      "\n",
      "The elephant looked and he said 'What are you doing?'\n",
      "The poor elephant looked down and said 'I'm pouring electricity!'\n",
      "The elephant looked out of the window and said 'Oh! That's why I look out to the sky, I'm able to help protect plants and plants from their worst fears.'\n",
      "\n",
      "The elephant smiled. With little help and thoughtfulness, he had saved the day\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"One day I was walking in an ancient desert when\"\n",
    "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf0b534-00a6-4f12-b2b0-2aea40bae3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a lion with a weird big nose. He likes to dance and sing! He dances in the dirt and the grass.\n",
      "\n",
      "One day, a little boy sees the lion. He loves lions and he wants to dance with him.\n",
      "\n",
      "The lion says, \"Come, little boy! dance with me here!\" The little boy is excited and says, \"Yes! Let's dance!\"\n",
      "\n",
      "So, the lion and the little boy go to the dirt and dance together. They dance loud and fast with their little nose! It makes them feel happy and funny.\n",
      "\n",
      "Soon, they are fast asleep and the lion and the little boy dance. They dance together in the dirt and the grass until the sun goes down.\n",
      "\n",
      "The little boy and the lion never catch another lion again. They dance happily together in the middle of the night.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a lion with a weird big nose\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f7d0967-1b60-4537-b61e-93ccbbb50f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a giraffe with a little neck. He liked to play in the sunshine. One day, the hill started to shake, and the giraffe felt very frightened. The other animals ran away from him.\n",
      "\n",
      "\"Help us, giraffe!\" one duck said. \"We must find a way past it.\"\n",
      "\n",
      "The other animals looked for a way, but they couldn't find one. The giraffe felt very sad and lonely. He started to yawn. \n",
      "\n",
      "The giraffe slowly walked back to his forest. As he walked, he noticed that some of his friends were trying to cross the bridge. They were shaking over and over again. The giraffe was not scared anymore. \n",
      "\n",
      "The giraffe stopped yawning and thought to himself, \"I can handle this alone. Maybe one day, another way will help us.\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a giraffe with a little neck\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91cdc6c4-0a85-4416-b5e5-42ed859006a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a leon with a cute tail. It was a rare leon because it always saw the sun shining on it. It was very happy with its tail and flew around in the sky.\n",
      "\n",
      "One day, the leon saw something bright on the ground. It saw a tiny bird. The leon thought the bird was nice, so it flew closer to it. The bird cooed with it and the leon saw that it was trying to learn.\n",
      "\n",
      "The leon felt happy that the bird liked it. The leon was happy too because it had a good friend and their love. They played together, and the leon taught the bird to like it and share its love with everyone.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a leon with a cute tail\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1ae4090-e888-48e6-8aef-64a4b9d021d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='275' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [275/275 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION SET EVALUATION RESULTS ===\n",
      "eval_loss: 0.5725\n",
      "eval_runtime: 40.0149\n",
      "eval_samples_per_second: 27.4650\n",
      "eval_steps_per_second: 6.8720\n",
      "epoch: 3.0000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate perplexity\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mexp(eval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Interpret model quality based on perplexity\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=== VALIDATION SET EVALUATION RESULTS ===\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d849c3a0-638e-4544-95b1-91071a5b48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1.77\n",
      "Model Quality: Excellent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(eval_results[\"eval_loss\"])\n",
    "print(f\"\\nPerplexity: {perplexity:.2f}\")\n",
    "\n",
    "# Interpret model quality based on perplexity\n",
    "if perplexity < 50:\n",
    "    quality = \"Excellent\"\n",
    "elif perplexity < 100:\n",
    "    quality = \"Good\"\n",
    "elif perplexity < 200:\n",
    "    quality = \"Fair\"\n",
    "else:\n",
    "    quality = \"Needs Improvement\"\n",
    "\n",
    "print(f\"Model Quality: {quality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fcaf5-cb1e-4396-a8b0-55cea746e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_distinct_n(texts, n=1):\n",
    "    ngrams = []\n",
    "    for text in texts:\n",
    "        tokens = text.strip().split()\n",
    "        ngrams.extend([tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "    total = len(ngrams)\n",
    "    unique = len(set(ngrams))\n",
    "    return unique / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2031cab-5614-44d0-b043-e1b0b1ff7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e5e95b-839d-40ba-8084-408eb18b7580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./venv/lib/python3.12/site-packages (from sentence_transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.12/site-packages (from sentence_transformers) (2.7.1)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Using cached scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./venv/lib/python3.12/site-packages (from sentence_transformers) (0.33.2)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./venv/lib/python3.12/site-packages (from sentence_transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.5)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.6.15)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, Pillow, joblib, scikit-learn, sentence_transformers\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [sentence_transformers]/6\u001b[0m [sentence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Pillow-11.3.0 joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 sentence_transformers-5.0.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6d384f-a854-4415-9ca5-51a4ab89308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model_path with your checkpoint directory\n",
    "model_path = \"/home/kiro/Downloads/Project/gpt2-tinystories3/checkpoint-79491\"\n",
    "\n",
    "dataset_name       = \"roneneldan/TinyStories\"\n",
    "split_name         = \"validation\"\n",
    "prompt_token_count = 20    # number of tokens to use as prompt\n",
    "num_generations    = 5     # number of stories per prompt for diversity\n",
    "max_gen_length     = 100   # max total tokens (prompt + generation)\n",
    "num_examples       = 300   # set None to evaluate all examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e048e380-83a8-4dad-ac2e-880af3655f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85cdd7fe-5630-4323-9b5c-7465b66b0d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█| 2119719/2119719 [00:03<00:00, 699680.85 examples\n",
      "Generating validation split: 100%|█| 21990/21990 [00:00<00:00, 707979.56 example\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(dataset_name, split=split_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f55847-7393-49bf-be25-a2162a1949c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores  = []\n",
    "overlap_scores = []\n",
    "diversity_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef9e47d-0229-4d2a-bfc0-55233852a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129/300] ✅ Len(prompt): 20 | Len(gold): 62 | Cosine: 0.5213 | Overlap: 0.4706 | Diversity: 0.3689\n",
      "[130/300] ✅ Len(prompt): 20 | Len(gold): 283 | Cosine: 0.2717 | Overlap: 0.4118 | Diversity: 0.3600\n",
      "[131/300] ✅ Len(prompt): 20 | Len(gold): 111 | Cosine: 0.5228 | Overlap: 0.5714 | Diversity: 0.3267\n",
      "[132/300] ✅ Len(prompt): 20 | Len(gold): 171 | Cosine: 0.5954 | Overlap: 0.5556 | Diversity: 0.3475\n",
      "[133/300] ✅ Len(prompt): 20 | Len(gold): 124 | Cosine: 0.6327 | Overlap: 0.6000 | Diversity: 0.3609\n",
      "[134/300] ✅ Len(prompt): 20 | Len(gold): 181 | Cosine: 0.4135 | Overlap: 0.4706 | Diversity: 0.3775\n",
      "[135/300] ✅ Len(prompt): 20 | Len(gold): 133 | Cosine: 0.4198 | Overlap: 0.5789 | Diversity: 0.3625\n",
      "[136/300] ✅ Len(prompt): 20 | Len(gold): 131 | Cosine: 0.3530 | Overlap: 0.5789 | Diversity: 0.3648\n",
      "[137/300] ✅ Len(prompt): 20 | Len(gold): 132 | Cosine: 0.5729 | Overlap: 0.5294 | Diversity: 0.3951\n",
      "[138/300] ✅ Len(prompt): 20 | Len(gold): 205 | Cosine: 0.6095 | Overlap: 0.6250 | Diversity: 0.3516\n",
      "[139/300] ✅ Len(prompt): 20 | Len(gold): 180 | Cosine: 0.5324 | Overlap: 0.6250 | Diversity: 0.3400\n",
      "[140/300] ✅ Len(prompt): 20 | Len(gold): 121 | Cosine: 0.6319 | Overlap: 0.4706 | Diversity: 0.3731\n",
      "[141/300] ✅ Len(prompt): 20 | Len(gold): 139 | Cosine: 0.6173 | Overlap: 0.5882 | Diversity: 0.3050\n",
      "[142/300] ✅ Len(prompt): 20 | Len(gold): 154 | Cosine: 0.3991 | Overlap: 0.4118 | Diversity: 0.3731\n",
      "[143/300] ✅ Len(prompt): 20 | Len(gold): 174 | Cosine: 0.5180 | Overlap: 0.5625 | Diversity: 0.3700\n",
      "[144/300] ✅ Len(prompt): 20 | Len(gold): 176 | Cosine: 0.5528 | Overlap: 0.6667 | Diversity: 0.3697\n",
      "[145/300] ✅ Len(prompt): 20 | Len(gold): 140 | Cosine: 0.5052 | Overlap: 0.5263 | Diversity: 0.4150\n",
      "[146/300] ✅ Len(prompt): 20 | Len(gold): 173 | Cosine: 0.5148 | Overlap: 0.3889 | Diversity: 0.3800\n",
      "[147/300] ✅ Len(prompt): 20 | Len(gold): 149 | Cosine: 0.4953 | Overlap: 0.5385 | Diversity: 0.3625\n",
      "[148/300] ✅ Len(prompt): 20 | Len(gold): 127 | Cosine: 0.5277 | Overlap: 0.5294 | Diversity: 0.3650\n",
      "[149/300] ✅ Len(prompt): 20 | Len(gold): 186 | Cosine: 0.4941 | Overlap: 0.6111 | Diversity: 0.3684\n",
      "[150/300] ✅ Len(prompt): 20 | Len(gold): 144 | Cosine: 0.5551 | Overlap: 0.6471 | Diversity: 0.3575\n",
      "[151/300] ✅ Len(prompt): 20 | Len(gold): 129 | Cosine: 0.4063 | Overlap: 0.4737 | Diversity: 0.4070\n",
      "[152/300] ✅ Len(prompt): 20 | Len(gold): 147 | Cosine: 0.5414 | Overlap: 0.4375 | Diversity: 0.3650\n",
      "[153/300] ✅ Len(prompt): 20 | Len(gold): 123 | Cosine: 0.4654 | Overlap: 0.3750 | Diversity: 0.3925\n",
      "[154/300] ✅ Len(prompt): 20 | Len(gold): 161 | Cosine: 0.5137 | Overlap: 0.6111 | Diversity: 0.4005\n",
      "[155/300] ✅ Len(prompt): 20 | Len(gold): 176 | Cosine: 0.6265 | Overlap: 0.7059 | Diversity: 0.3800\n",
      "[156/300] ✅ Len(prompt): 20 | Len(gold): 121 | Cosine: 0.4601 | Overlap: 0.5500 | Diversity: 0.3500\n",
      "[157/300] ✅ Len(prompt): 20 | Len(gold): 143 | Cosine: 0.4044 | Overlap: 0.5789 | Diversity: 0.3000\n",
      "[158/300] ✅ Len(prompt): 20 | Len(gold): 159 | Cosine: 0.5021 | Overlap: 0.5789 | Diversity: 0.3400\n",
      "[159/300] ✅ Len(prompt): 20 | Len(gold): 113 | Cosine: 0.6391 | Overlap: 0.5294 | Diversity: 0.3350\n",
      "[160/300] ✅ Len(prompt): 20 | Len(gold): 128 | Cosine: 0.6107 | Overlap: 0.6471 | Diversity: 0.3688\n",
      "[161/300] ✅ Len(prompt): 20 | Len(gold): 143 | Cosine: 0.5016 | Overlap: 0.5789 | Diversity: 0.3300\n",
      "[162/300] ✅ Len(prompt): 20 | Len(gold): 208 | Cosine: 0.4912 | Overlap: 0.6111 | Diversity: 0.3775\n",
      "[163/300] ✅ Len(prompt): 20 | Len(gold): 117 | Cosine: 0.5072 | Overlap: 0.2778 | Diversity: 0.4005\n",
      "[164/300] ✅ Len(prompt): 20 | Len(gold): 116 | Cosine: 0.4294 | Overlap: 0.5000 | Diversity: 0.4115\n",
      "[165/300] ✅ Len(prompt): 20 | Len(gold): 116 | Cosine: 0.7477 | Overlap: 0.6316 | Diversity: 0.3600\n",
      "[166/300] ✅ Len(prompt): 20 | Len(gold): 143 | Cosine: 0.6466 | Overlap: 0.6667 | Diversity: 0.3759\n",
      "[167/300] ✅ Len(prompt): 20 | Len(gold): 136 | Cosine: 0.5852 | Overlap: 0.3889 | Diversity: 0.4115\n",
      "[168/300] ✅ Len(prompt): 20 | Len(gold): 210 | Cosine: 0.4910 | Overlap: 0.5882 | Diversity: 0.3975\n",
      "[169/300] ✅ Len(prompt): 20 | Len(gold): 160 | Cosine: 0.6239 | Overlap: 0.3333 | Diversity: 0.3767\n",
      "[170/300] ✅ Len(prompt): 20 | Len(gold): 163 | Cosine: 0.4661 | Overlap: 0.4706 | Diversity: 0.3450\n",
      "[171/300] ✅ Len(prompt): 20 | Len(gold): 173 | Cosine: 0.5146 | Overlap: 0.3684 | Diversity: 0.4025\n",
      "[172/300] ✅ Len(prompt): 20 | Len(gold): 103 | Cosine: 0.5392 | Overlap: 0.4444 | Diversity: 0.4160\n",
      "[173/300] ✅ Len(prompt): 20 | Len(gold): 165 | Cosine: 0.5416 | Overlap: 0.5294 | Diversity: 0.3475\n",
      "[174/300] ✅ Len(prompt): 20 | Len(gold): 134 | Cosine: 0.6023 | Overlap: 0.6111 | Diversity: 0.4000\n",
      "[175/300] ✅ Len(prompt): 20 | Len(gold): 81 | Cosine: 0.5230 | Overlap: 0.4706 | Diversity: 0.3929\n",
      "[176/300] ✅ Len(prompt): 20 | Len(gold): 160 | Cosine: 0.5458 | Overlap: 0.5263 | Diversity: 0.3800\n",
      "[177/300] ✅ Len(prompt): 20 | Len(gold): 147 | Cosine: 0.4893 | Overlap: 0.4444 | Diversity: 0.3550\n",
      "[178/300] ✅ Len(prompt): 20 | Len(gold): 172 | Cosine: 0.5547 | Overlap: 0.3333 | Diversity: 0.3775\n",
      "[179/300] ✅ Len(prompt): 20 | Len(gold): 173 | Cosine: 0.4619 | Overlap: 0.5294 | Diversity: 0.4005\n",
      "[180/300] ✅ Len(prompt): 20 | Len(gold): 157 | Cosine: 0.4277 | Overlap: 0.6471 | Diversity: 0.3604\n",
      "[181/300] ✅ Len(prompt): 20 | Len(gold): 164 | Cosine: 0.4084 | Overlap: 0.4444 | Diversity: 0.4143\n",
      "[182/300] ✅ Len(prompt): 20 | Len(gold): 139 | Cosine: 0.4469 | Overlap: 0.5294 | Diversity: 0.4115\n",
      "[183/300] ✅ Len(prompt): 20 | Len(gold): 142 | Cosine: 0.6223 | Overlap: 0.6471 | Diversity: 0.3854\n",
      "[184/300] ✅ Len(prompt): 20 | Len(gold): 157 | Cosine: 0.4343 | Overlap: 0.4737 | Diversity: 0.3940\n",
      "[185/300] ✅ Len(prompt): 20 | Len(gold): 134 | Cosine: 0.3838 | Overlap: 0.5263 | Diversity: 0.4040\n",
      "[186/300] ✅ Len(prompt): 20 | Len(gold): 186 | Cosine: 0.5755 | Overlap: 0.6471 | Diversity: 0.3800\n",
      "[187/300] ✅ Len(prompt): 20 | Len(gold): 175 | Cosine: 0.4258 | Overlap: 0.5882 | Diversity: 0.3725\n",
      "[188/300] ✅ Len(prompt): 20 | Len(gold): 212 | Cosine: 0.4773 | Overlap: 0.5789 | Diversity: 0.4200\n",
      "[189/300] ✅ Len(prompt): 20 | Len(gold): 156 | Cosine: 0.4950 | Overlap: 0.6316 | Diversity: 0.3500\n",
      "[190/300] ✅ Len(prompt): 20 | Len(gold): 87 | Cosine: 0.4906 | Overlap: 0.3529 | Diversity: 0.3425\n",
      "[191/300] ✅ Len(prompt): 20 | Len(gold): 120 | Cosine: 0.4775 | Overlap: 0.5263 | Diversity: 0.3800\n",
      "[192/300] ✅ Len(prompt): 20 | Len(gold): 200 | Cosine: 0.5110 | Overlap: 0.5263 | Diversity: 0.3700\n",
      "[193/300] ✅ Len(prompt): 20 | Len(gold): 170 | Cosine: 0.4836 | Overlap: 0.4706 | Diversity: 0.3825\n",
      "[194/300] ✅ Len(prompt): 20 | Len(gold): 29 | Cosine: 0.2899 | Overlap: 0.2222 | Diversity: 0.4276\n",
      "[195/300] ✅ Len(prompt): 20 | Len(gold): 129 | Cosine: 0.4640 | Overlap: 0.4375 | Diversity: 0.3450\n",
      "[196/300] ✅ Len(prompt): 20 | Len(gold): 88 | Cosine: 0.6815 | Overlap: 0.4444 | Diversity: 0.3850\n",
      "[197/300] ✅ Len(prompt): 20 | Len(gold): 185 | Cosine: 0.3020 | Overlap: 0.3529 | Diversity: 0.3747\n",
      "[198/300] ✅ Len(prompt): 20 | Len(gold): 143 | Cosine: 0.5322 | Overlap: 0.5556 | Diversity: 0.4069\n",
      "[199/300] ✅ Len(prompt): 20 | Len(gold): 117 | Cosine: 0.4582 | Overlap: 0.5789 | Diversity: 0.4000\n",
      "[200/300] ✅ Len(prompt): 20 | Len(gold): 151 | Cosine: 0.5026 | Overlap: 0.4706 | Diversity: 0.3678\n",
      "[201/300] ✅ Len(prompt): 20 | Len(gold): 140 | Cosine: 0.6191 | Overlap: 0.5882 | Diversity: 0.3475\n",
      "[202/300] ✅ Len(prompt): 20 | Len(gold): 199 | Cosine: 0.5141 | Overlap: 0.5789 | Diversity: 0.3825\n",
      "[203/300] ✅ Len(prompt): 20 | Len(gold): 230 | Cosine: 0.4418 | Overlap: 0.5294 | Diversity: 0.3744\n",
      "[204/300] ✅ Len(prompt): 20 | Len(gold): 180 | Cosine: 0.5487 | Overlap: 0.6111 | Diversity: 0.4025\n",
      "[205/300] ✅ Len(prompt): 20 | Len(gold): 211 | Cosine: 0.4281 | Overlap: 0.5263 | Diversity: 0.3125\n",
      "[206/300] ✅ Len(prompt): 20 | Len(gold): 168 | Cosine: 0.5979 | Overlap: 0.5556 | Diversity: 0.4225\n",
      "[207/300] ✅ Len(prompt): 20 | Len(gold): 137 | Cosine: 0.6341 | Overlap: 0.5000 | Diversity: 0.3500\n",
      "[208/300] ✅ Len(prompt): 20 | Len(gold): 97 | Cosine: 0.6515 | Overlap: 0.5333 | Diversity: 0.3049\n",
      "[209/300] ✅ Len(prompt): 20 | Len(gold): 164 | Cosine: 0.4382 | Overlap: 0.5263 | Diversity: 0.3042\n",
      "[210/300] ✅ Len(prompt): 20 | Len(gold): 156 | Cosine: 0.5974 | Overlap: 0.4706 | Diversity: 0.3914\n",
      "[211/300] ✅ Len(prompt): 20 | Len(gold): 144 | Cosine: 0.4627 | Overlap: 0.4706 | Diversity: 0.3500\n",
      "[212/300] ✅ Len(prompt): 20 | Len(gold): 153 | Cosine: 0.6580 | Overlap: 0.7222 | Diversity: 0.4025\n",
      "[213/300] ✅ Len(prompt): 20 | Len(gold): 189 | Cosine: 0.5890 | Overlap: 0.5789 | Diversity: 0.4085\n",
      "[214/300] ✅ Len(prompt): 20 | Len(gold): 135 | Cosine: 0.5903 | Overlap: 0.3889 | Diversity: 0.3625\n",
      "[215/300] ✅ Len(prompt): 20 | Len(gold): 160 | Cosine: 0.4389 | Overlap: 0.4706 | Diversity: 0.4125\n",
      "[216/300] ✅ Len(prompt): 20 | Len(gold): 129 | Cosine: 0.4904 | Overlap: 0.4375 | Diversity: 0.4051\n",
      "[217/300] ✅ Len(prompt): 20 | Len(gold): 147 | Cosine: 0.4980 | Overlap: 0.5000 | Diversity: 0.3881\n",
      "[218/300] ✅ Len(prompt): 20 | Len(gold): 141 | Cosine: 0.3531 | Overlap: 0.3684 | Diversity: 0.4020\n",
      "[219/300] ✅ Len(prompt): 20 | Len(gold): 162 | Cosine: 0.2116 | Overlap: 0.6316 | Diversity: 0.3850\n",
      "[220/300] ✅ Len(prompt): 20 | Len(gold): 189 | Cosine: 0.4987 | Overlap: 0.5263 | Diversity: 0.4000\n",
      "[221/300] ✅ Len(prompt): 20 | Len(gold): 189 | Cosine: 0.3745 | Overlap: 0.6316 | Diversity: 0.4049\n",
      "[222/300] ✅ Len(prompt): 20 | Len(gold): 178 | Cosine: 0.5306 | Overlap: 0.6316 | Diversity: 0.3725\n",
      "[223/300] ✅ Len(prompt): 20 | Len(gold): 71 | Cosine: 0.5459 | Overlap: 0.5294 | Diversity: 0.3983\n",
      "[224/300] ✅ Len(prompt): 20 | Len(gold): 143 | Cosine: 0.4466 | Overlap: 0.5294 | Diversity: 0.3490\n",
      "[225/300] ✅ Len(prompt): 20 | Len(gold): 193 | Cosine: 0.4698 | Overlap: 0.6667 | Diversity: 0.4010\n",
      "[226/300] ✅ Len(prompt): 20 | Len(gold): 223 | Cosine: 0.4587 | Overlap: 0.5789 | Diversity: 0.3975\n",
      "[227/300] ✅ Len(prompt): 20 | Len(gold): 149 | Cosine: 0.5926 | Overlap: 0.5263 | Diversity: 0.3400\n",
      "[228/300] ✅ Len(prompt): 20 | Len(gold): 133 | Cosine: 0.4095 | Overlap: 0.5789 | Diversity: 0.3865\n",
      "[229/300] ✅ Len(prompt): 20 | Len(gold): 146 | Cosine: 0.5628 | Overlap: 0.5294 | Diversity: 0.3325\n",
      "[230/300] ✅ Len(prompt): 20 | Len(gold): 106 | Cosine: 0.6465 | Overlap: 0.5789 | Diversity: 0.3500\n",
      "[231/300] ✅ Len(prompt): 20 | Len(gold): 163 | Cosine: 0.5200 | Overlap: 0.4737 | Diversity: 0.3804\n",
      "[232/300] ✅ Len(prompt): 20 | Len(gold): 177 | Cosine: 0.5359 | Overlap: 0.5556 | Diversity: 0.3775\n",
      "[233/300] ✅ Len(prompt): 20 | Len(gold): 164 | Cosine: 0.3826 | Overlap: 0.4211 | Diversity: 0.3850\n",
      "[234/300] ✅ Len(prompt): 20 | Len(gold): 153 | Cosine: 0.4636 | Overlap: 0.6316 | Diversity: 0.3725\n",
      "[235/300] ✅ Len(prompt): 20 | Len(gold): 101 | Cosine: 0.5319 | Overlap: 0.5625 | Diversity: 0.3875\n",
      "[236/300] ✅ Len(prompt): 20 | Len(gold): 169 | Cosine: 0.3888 | Overlap: 0.5294 | Diversity: 0.3400\n",
      "[237/300] ✅ Len(prompt): 20 | Len(gold): 155 | Cosine: 0.6639 | Overlap: 0.5263 | Diversity: 0.3450\n",
      "[238/300] ✅ Len(prompt): 20 | Len(gold): 151 | Cosine: 0.5303 | Overlap: 0.5625 | Diversity: 0.3625\n",
      "[239/300] ✅ Len(prompt): 20 | Len(gold): 128 | Cosine: 0.4925 | Overlap: 0.4118 | Diversity: 0.3929\n",
      "[240/300] ✅ Len(prompt): 20 | Len(gold): 116 | Cosine: 0.4194 | Overlap: 0.4737 | Diversity: 0.3466\n",
      "[241/300] ✅ Len(prompt): 20 | Len(gold): 195 | Cosine: 0.6681 | Overlap: 0.5263 | Diversity: 0.4050\n",
      "[242/300] ✅ Len(prompt): 20 | Len(gold): 178 | Cosine: 0.4975 | Overlap: 0.5294 | Diversity: 0.3625\n",
      "[243/300] ✅ Len(prompt): 20 | Len(gold): 198 | Cosine: 0.4416 | Overlap: 0.5789 | Diversity: 0.3775\n",
      "[244/300] ✅ Len(prompt): 20 | Len(gold): 126 | Cosine: 0.5281 | Overlap: 0.4706 | Diversity: 0.3643\n",
      "[245/300] ✅ Len(prompt): 20 | Len(gold): 141 | Cosine: 0.6307 | Overlap: 0.6250 | Diversity: 0.3856\n",
      "[246/300] ✅ Len(prompt): 20 | Len(gold): 151 | Cosine: 0.2995 | Overlap: 0.5000 | Diversity: 0.3675\n",
      "[247/300] ✅ Len(prompt): 20 | Len(gold): 152 | Cosine: 0.3911 | Overlap: 0.5000 | Diversity: 0.3875\n",
      "[248/300] ✅ Len(prompt): 20 | Len(gold): 138 | Cosine: 0.4870 | Overlap: 0.3889 | Diversity: 0.3800\n",
      "[249/300] ✅ Len(prompt): 20 | Len(gold): 154 | Cosine: 0.5043 | Overlap: 0.5882 | Diversity: 0.4127\n",
      "[250/300] ✅ Len(prompt): 20 | Len(gold): 181 | Cosine: 0.5441 | Overlap: 0.6250 | Diversity: 0.3850\n",
      "[251/300] ✅ Len(prompt): 20 | Len(gold): 129 | Cosine: 0.4602 | Overlap: 0.5625 | Diversity: 0.3750\n",
      "[252/300] ✅ Len(prompt): 20 | Len(gold): 119 | Cosine: 0.4974 | Overlap: 0.3684 | Diversity: 0.3650\n",
      "[253/300] ✅ Len(prompt): 20 | Len(gold): 202 | Cosine: 0.4833 | Overlap: 0.4706 | Diversity: 0.3450\n",
      "[254/300] ✅ Len(prompt): 20 | Len(gold): 140 | Cosine: 0.4499 | Overlap: 0.4706 | Diversity: 0.4075\n",
      "[255/300] ✅ Len(prompt): 20 | Len(gold): 130 | Cosine: 0.6182 | Overlap: 0.3529 | Diversity: 0.3900\n",
      "[256/300] ✅ Len(prompt): 20 | Len(gold): 203 | Cosine: 0.5279 | Overlap: 0.8125 | Diversity: 0.4190\n",
      "[257/300] ✅ Len(prompt): 20 | Len(gold): 189 | Cosine: 0.3158 | Overlap: 0.4211 | Diversity: 0.3925\n",
      "[258/300] ✅ Len(prompt): 20 | Len(gold): 90 | Cosine: 0.6427 | Overlap: 0.4118 | Diversity: 0.3425\n",
      "[259/300] ✅ Len(prompt): 20 | Len(gold): 141 | Cosine: 0.4520 | Overlap: 0.5294 | Diversity: 0.3836\n",
      "[260/300] ✅ Len(prompt): 20 | Len(gold): 136 | Cosine: 0.6197 | Overlap: 0.5263 | Diversity: 0.3950\n",
      "[261/300] ✅ Len(prompt): 20 | Len(gold): 130 | Cosine: 0.6590 | Overlap: 0.5263 | Diversity: 0.3879\n",
      "[262/300] ✅ Len(prompt): 20 | Len(gold): 156 | Cosine: 0.5357 | Overlap: 0.5000 | Diversity: 0.3925\n",
      "[263/300] ✅ Len(prompt): 20 | Len(gold): 136 | Cosine: 0.5014 | Overlap: 0.4211 | Diversity: 0.3800\n",
      "[264/300] ✅ Len(prompt): 20 | Len(gold): 141 | Cosine: 0.4828 | Overlap: 0.6667 | Diversity: 0.3317\n",
      "[265/300] ✅ Len(prompt): 20 | Len(gold): 160 | Cosine: 0.4253 | Overlap: 0.4211 | Diversity: 0.3775\n",
      "[266/300] ✅ Len(prompt): 20 | Len(gold): 220 | Cosine: 0.5276 | Overlap: 0.5000 | Diversity: 0.3450\n",
      "[267/300] ✅ Len(prompt): 20 | Len(gold): 165 | Cosine: 0.5002 | Overlap: 0.5556 | Diversity: 0.3432\n",
      "[268/300] ✅ Len(prompt): 20 | Len(gold): 119 | Cosine: 0.3680 | Overlap: 0.4737 | Diversity: 0.3625\n",
      "[269/300] ✅ Len(prompt): 20 | Len(gold): 211 | Cosine: 0.5181 | Overlap: 0.5882 | Diversity: 0.4093\n",
      "[270/300] ✅ Len(prompt): 20 | Len(gold): 162 | Cosine: 0.3769 | Overlap: 0.5000 | Diversity: 0.3250\n",
      "[271/300] ✅ Len(prompt): 20 | Len(gold): 159 | Cosine: 0.5707 | Overlap: 0.5556 | Diversity: 0.3955\n",
      "[272/300] ✅ Len(prompt): 20 | Len(gold): 140 | Cosine: 0.6435 | Overlap: 0.5556 | Diversity: 0.3975\n",
      "[273/300] ✅ Len(prompt): 20 | Len(gold): 174 | Cosine: 0.5023 | Overlap: 0.5263 | Diversity: 0.3850\n",
      "[274/300] ✅ Len(prompt): 20 | Len(gold): 172 | Cosine: 0.3945 | Overlap: 0.5294 | Diversity: 0.4025\n",
      "[275/300] ✅ Len(prompt): 20 | Len(gold): 191 | Cosine: 0.4595 | Overlap: 0.4118 | Diversity: 0.4175\n",
      "[276/300] ✅ Len(prompt): 20 | Len(gold): 156 | Cosine: 0.5174 | Overlap: 0.7059 | Diversity: 0.3738\n",
      "[277/300] ✅ Len(prompt): 20 | Len(gold): 126 | Cosine: 0.6215 | Overlap: 0.4706 | Diversity: 0.3700\n",
      "[278/300] ✅ Len(prompt): 20 | Len(gold): 191 | Cosine: 0.2940 | Overlap: 0.4211 | Diversity: 0.3802\n",
      "[279/300] ✅ Len(prompt): 20 | Len(gold): 151 | Cosine: 0.4598 | Overlap: 0.4667 | Diversity: 0.3650\n",
      "[280/300] ✅ Len(prompt): 20 | Len(gold): 102 | Cosine: 0.5149 | Overlap: 0.6875 | Diversity: 0.3575\n",
      "[281/300] ✅ Len(prompt): 20 | Len(gold): 196 | Cosine: 0.5722 | Overlap: 0.5556 | Diversity: 0.3750\n",
      "[282/300] ✅ Len(prompt): 20 | Len(gold): 191 | Cosine: 0.5436 | Overlap: 0.4375 | Diversity: 0.3350\n",
      "[283/300] ✅ Len(prompt): 20 | Len(gold): 172 | Cosine: 0.6556 | Overlap: 0.5882 | Diversity: 0.3475\n",
      "[284/300] ✅ Len(prompt): 20 | Len(gold): 139 | Cosine: 0.5779 | Overlap: 0.5000 | Diversity: 0.3772\n",
      "[285/300] ✅ Len(prompt): 20 | Len(gold): 129 | Cosine: 0.5626 | Overlap: 0.4737 | Diversity: 0.4196\n",
      "[286/300] ✅ Len(prompt): 20 | Len(gold): 174 | Cosine: 0.5281 | Overlap: 0.5882 | Diversity: 0.3300\n",
      "[287/300] ✅ Len(prompt): 20 | Len(gold): 127 | Cosine: 0.4094 | Overlap: 0.4737 | Diversity: 0.3548\n",
      "[288/300] ✅ Len(prompt): 20 | Len(gold): 123 | Cosine: 0.4320 | Overlap: 0.6667 | Diversity: 0.4025\n",
      "[289/300] ✅ Len(prompt): 20 | Len(gold): 167 | Cosine: 0.4281 | Overlap: 0.5263 | Diversity: 0.3550\n",
      "[290/300] ✅ Len(prompt): 20 | Len(gold): 42 | Cosine: 0.7546 | Overlap: 0.5000 | Diversity: 0.4571\n",
      "[291/300] ✅ Len(prompt): 20 | Len(gold): 211 | Cosine: 0.4090 | Overlap: 0.4737 | Diversity: 0.3550\n",
      "[292/300] ✅ Len(prompt): 20 | Len(gold): 234 | Cosine: 0.4180 | Overlap: 0.5556 | Diversity: 0.3955\n",
      "[293/300] ✅ Len(prompt): 20 | Len(gold): 235 | Cosine: 0.5737 | Overlap: 0.5294 | Diversity: 0.4071\n",
      "[294/300] ✅ Len(prompt): 20 | Len(gold): 154 | Cosine: 0.5039 | Overlap: 0.5882 | Diversity: 0.3500\n",
      "[295/300] ✅ Len(prompt): 20 | Len(gold): 141 | Cosine: 0.5168 | Overlap: 0.5294 | Diversity: 0.3769\n",
      "[296/300] ✅ Len(prompt): 20 | Len(gold): 170 | Cosine: 0.6102 | Overlap: 0.5294 | Diversity: 0.4040\n",
      "[297/300] ✅ Len(prompt): 20 | Len(gold): 147 | Cosine: 0.4919 | Overlap: 0.4737 | Diversity: 0.4204\n",
      "[298/300] ✅ Len(prompt): 20 | Len(gold): 164 | Cosine: 0.5022 | Overlap: 0.5556 | Diversity: 0.3375\n",
      "[299/300] ✅ Len(prompt): 20 | Len(gold): 149 | Cosine: 0.4397 | Overlap: 0.5789 | Diversity: 0.3955\n",
      "[300/300] ✅ Len(prompt): 20 | Len(gold): 118 | Cosine: 0.4732 | Overlap: 0.5294 | Diversity: 0.4250\n",
      "✅ Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "for idx, example in enumerate(ds):\n",
    "    text = example[\"text\"].strip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    if len(tokens) <= prompt_token_count + 1:\n",
    "        continue\n",
    "\n",
    "    prompt_tokens = tokens[:prompt_token_count]\n",
    "    gold_tokens   = tokens[prompt_token_count:]\n",
    "    prompt = tokenizer.convert_tokens_to_string(prompt_tokens)\n",
    "    gold   = tokenizer.convert_tokens_to_string(gold_tokens)\n",
    "\n",
    "    # Tokenize prompt and move to GPU\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "    # Generate sequences\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=min(len(input_ids[0]) + len(gold_tokens), max_gen_length),\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=num_generations,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Decode generated texts\n",
    "    gens = []\n",
    "    for gen in output_ids:\n",
    "        gen_tok = gen[input_ids.shape[1]:].tolist()\n",
    "        gens.append(tokenizer.decode(gen_tok, skip_special_tokens=True).strip())\n",
    "\n",
    "    # Cosine similarity\n",
    "    emb_prompt = embedder.encode([prompt])\n",
    "    emb_gold   = embedder.encode([gold])\n",
    "    cs = float(cosine_similarity(emb_prompt, emb_gold)[0, 0])\n",
    "    cosine_scores.append(cs)\n",
    "\n",
    "    # Unigram overlap\n",
    "    set_prompt = set(prompt_tokens)\n",
    "    set_gold   = set(tokenizer.tokenize(gold))\n",
    "    overlap = len(set_prompt & set_gold) / len(set_prompt)\n",
    "    overlap_scores.append(overlap)\n",
    "\n",
    "    # Diversity (distinct-1)\n",
    "    all_gen_toks = []\n",
    "    for g in gens:\n",
    "        all_gen_toks.extend(tokenizer.tokenize(g))\n",
    "    diversity = len(set(all_gen_toks)) / len(all_gen_toks) if all_gen_toks else 0.0\n",
    "    diversity_scores.append(diversity)\n",
    "\n",
    "    # 🔄 Progress update\n",
    "    print(f\"[{len(cosine_scores)}/{num_examples or '∞'}] ✅ \"\n",
    "          f\"Len(prompt): {len(prompt_tokens)} | Len(gold): {len(gold_tokens)} | \"\n",
    "          f\"Cosine: {cs:.4f} | Overlap: {overlap:.4f} | Diversity: {diversity:.4f}\")\n",
    "\n",
    "    if num_examples and len(cosine_scores) >= num_examples:\n",
    "        print(\"✅ Evaluation complete.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c1f94df-77b6-4db2-a790-88332a883df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Cosine similarity (prompt vs gold): mean=0.5066, std=0.0914\n",
      "Unigram overlap  (prompt vs gold): mean=0.5249, std=0.0916\n",
      "Diversity (distinct-1): mean=0.3763, std=0.0278\n"
     ]
    }
   ],
   "source": [
    "def summarize(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    return f\"{name}: mean={arr.mean():.4f}, std={arr.std():.4f}\"\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(summarize(\"Cosine similarity (prompt vs gold)\", cosine_scores))\n",
    "print(summarize(\"Unigram overlap  (prompt vs gold)\", overlap_scores))\n",
    "print(summarize(\"Diversity (distinct-1)\", diversity_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ff7996-8ca5-4f0e-a20f-011c52c50b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Completions ===\n",
      "\n",
      "[1] how to fly. He asked his mom, \"Can you teach me how to fly?\" His mom said, \"Sure, let's practice together.\" \n",
      "\n",
      "As they practiced, the robot started to fly. He started to climb trees and swing on the branches. He flew so high that he felt like he was flying!\n",
      "\n",
      "He tried to stay focused so he could fly. He even got to the other side of a big hill\n",
      "\n",
      "[2] how to play. He asked his friends all over the world if he could join them, but he still couldn't figure out how. He wanted to start and he was worried.\n",
      "\n",
      "But one day he got an idea. He asked his friends in the garden to try and teach him. His friends were happy to help and together they began to teach the little robot how to play.\n",
      "\n",
      "The little robot was amazed by what they were\n",
      "\n",
      "[3] how to cook. So, it asked its friend, a wise owl, if it could help. The owl said yes, and the robot learned how to make it very happy.\n",
      "\n",
      "But, the robot was not happy with the robot. The robot wanted to keep trying, but it realized it was no use trying. So, it decided to hide the robot and hide it under a tree. The owl did not see it, so the\n",
      "\n",
      "[4] how to fly. The robot was a very smart robot. It had a very big brain. The robot wanted to learn how to fly. So, it tried to learn what to do.\n",
      "\n",
      "One day, the robot was flying when it saw a big cloud. The cloud was very big. The robot could not see well. The robot was sad. It wanted to learn how to fly. But it could not do it alone. The\n",
      "\n",
      "[5] how to play. He asked his robot friend, \"Can you teach me how to play?\" The robot said, \"Sure, I can show you how!\"\n",
      "\n",
      "The robot and the robot played together all day. They had so much fun. They laughed and talked. They learned to count, to whistle, and to zip their hands.\n",
      "\n",
      "One day, the robot and the robot were in the kitchen. The robot saw a lot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace with any custom prompt\n",
    "your_prompt = \"Once upon a time, a little robot wanted to learn\"\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "encoded = tokenizer(your_prompt, return_tensors=\"pt\", padding=True)\n",
    "input_ids = encoded.input_ids.to(device)\n",
    "attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "# Generate sequences\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=100,                # total tokens including prompt\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=5,        # generate 5 completions\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Decode and show generations\n",
    "print(\"=== Generated Completions ===\\n\")\n",
    "for i, gen in enumerate(output_ids):\n",
    "    generated = tokenizer.decode(gen[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"[{i+1}] {generated.strip()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aad6a612-8124-40a7-bb13-c5fe237920e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 1099/1099 [00:00<00:00, 1239.11 examples/s]\n",
      "Perplexity Eval: 100%|██████████████████████| 1099/1099 [00:27<00:00, 39.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Checkpoint Perplexity (on 5% val): 1.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model from final checkpoint\n",
    "checkpoint_path = \"/home/kiro/Downloads/Project/gpt2-tinystories3/checkpoint-79491\"  # adjust to actual final step\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Sample same 5% val split\n",
    "val_full = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "val_data = val_full.shuffle(seed=123).select(range(len(val_full)//20))\n",
    "\n",
    "# Tokenize exactly like training\n",
    "def tokenize_function_with_labels(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "val_dataset = val_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Perplexity calculation\n",
    "def compute_perplexity(dataset, model):\n",
    "    losses = []\n",
    "    for example in tqdm(dataset, desc=\"Perplexity Eval\"):\n",
    "        input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(model.device)\n",
    "        attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(model.device)\n",
    "        labels = torch.tensor(example[\"labels\"]).unsqueeze(0).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "        losses.append(loss.item())\n",
    "    return np.exp(np.mean(losses))\n",
    "\n",
    "ppl = compute_perplexity(val_dataset, model)\n",
    "print(f\"\\n✅ Final Checkpoint Perplexity (on 5% val): {ppl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd0ef798-4480-46c6-8b20-ca24f5db48a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|█▏                           | 99/2489 [01:31<36:50,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Cosine similarity (generated vs gold): mean=0.6163, std=0.0749\n",
      "Unigram overlap (generated vs gold): mean=0.3827, std=0.0661\n",
      "Diversity (distinct-1 within generations): mean=0.5260, std=0.0290\n",
      "\n",
      "=== Additional Story Generation Metrics ===\n",
      "Total examples evaluated: 100\n",
      "Average generation length: 150.2 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "checkpoint_path = \"/home/kiro/Downloads/Project/gpt2-tinystories3/checkpoint-79491\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path).to(device).eval()\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Load 5% validation set\n",
    "val_data = load_dataset(\"roneneldan/TinyStories\", split=\"validation\").shuffle(seed=123).select(range(2489))\n",
    "\n",
    "# Parameters\n",
    "prompt_token_count = 20\n",
    "num_generations = 5\n",
    "max_gen_length = 150  # Fixed generation length, not dependent on gold text\n",
    "num_examples = 100\n",
    "\n",
    "# Metrics\n",
    "cosine_scores, overlap_scores, diversity_scores = [], [], []\n",
    "\n",
    "for i, example in enumerate(tqdm(val_data, desc=\"Evaluating\")):\n",
    "    text = example['text'].strip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if len(tokens) <= prompt_token_count + 1:\n",
    "        continue\n",
    "    \n",
    "    prompt_tokens = tokens[:prompt_token_count]\n",
    "    gold_tokens = tokens[prompt_token_count:]\n",
    "    prompt = tokenizer.convert_tokens_to_string(prompt_tokens)\n",
    "    gold = tokenizer.convert_tokens_to_string(gold_tokens)\n",
    "    \n",
    "    # Generate text\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    attention_mask = enc.attention_mask.to(device)\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=len(input_ids[0]) + max_gen_length,  # Fixed generation length\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=num_generations,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    gens = [tokenizer.decode(g[input_ids.shape[1]:], skip_special_tokens=True).strip() for g in output_ids]\n",
    "    \n",
    "    # 1. Cosine similarity: Compare GENERATED text vs GOLD text\n",
    "    gen_cosine_scores = []\n",
    "    for gen in gens:\n",
    "        if gen.strip():  # Only process non-empty generations\n",
    "            emb_gen = embedder.encode([gen])\n",
    "            emb_gold = embedder.encode([gold])\n",
    "            cosine = float(cosine_similarity(emb_gen, emb_gold)[0, 0])\n",
    "            gen_cosine_scores.append(cosine)\n",
    "    \n",
    "    if gen_cosine_scores:\n",
    "        cosine_scores.append(np.mean(gen_cosine_scores))  # Average across generations\n",
    "    \n",
    "    # 2. Unigram overlap: Compare GENERATED text vs GOLD text\n",
    "    gen_overlap_scores = []\n",
    "    gold_token_set = set(tokenizer.tokenize(gold))\n",
    "    for gen in gens:\n",
    "        if gen.strip():\n",
    "            gen_token_set = set(tokenizer.tokenize(gen))\n",
    "            if gen_token_set:  # Avoid division by zero\n",
    "                overlap = len(gen_token_set & gold_token_set) / len(gen_token_set)\n",
    "                gen_overlap_scores.append(overlap)\n",
    "    \n",
    "    if gen_overlap_scores:\n",
    "        overlap_scores.append(np.mean(gen_overlap_scores))  # Average across generations\n",
    "    \n",
    "    # 3. Diversity: Measure diversity WITHIN each generation, then average\n",
    "    gen_diversity_scores = []\n",
    "    for gen in gens:\n",
    "        if gen.strip():\n",
    "            gen_tokens = tokenizer.tokenize(gen)\n",
    "            if gen_tokens:  # Avoid division by zero\n",
    "                diversity = len(set(gen_tokens)) / len(gen_tokens)\n",
    "                gen_diversity_scores.append(diversity)\n",
    "    \n",
    "    if gen_diversity_scores:\n",
    "        diversity_scores.append(np.mean(gen_diversity_scores))  # Average across generations\n",
    "    \n",
    "    if len(cosine_scores) >= num_examples:\n",
    "        break\n",
    "\n",
    "def summarize(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    return f\"{name}: mean={arr.mean():.4f}, std={arr.std():.4f}\"\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(summarize(\"Cosine similarity (generated vs gold)\", cosine_scores))\n",
    "print(summarize(\"Unigram overlap (generated vs gold)\", overlap_scores))\n",
    "print(summarize(\"Diversity (distinct-1 within generations)\", diversity_scores))\n",
    "\n",
    "# Additional useful metrics for story generation\n",
    "print(\"\\n=== Additional Story Generation Metrics ===\")\n",
    "print(f\"Total examples evaluated: {len(cosine_scores)}\")\n",
    "print(f\"Average generation length: {np.mean([len(tokenizer.tokenize(g)) for example in val_data[:num_examples] for g in gens if g.strip()]):.1f} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c87b873e-55e2-4521-9fa2-9d56608a17ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 4398/4398 [00:03<00:00, 1197.64 examples/s]\n",
      "Perplexity: 100%|███████████████████████████| 4398/4398 [01:58<00:00, 37.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final-checkpoint PPL on new 5 % slice: 1.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Re-measure perplexity on a fresh 5 % random slice of TinyStories-validation.\n",
    "\n",
    "checkpoint_path = \"/home/kiro/Downloads/Project/gpt2-tinystories3/checkpoint-79491\"\n",
    "rnd_seed        = 123                       # change to try another shuffle\n",
    "slice_pct       = 0.2                      # 5 % of the full validation set\n",
    "max_len         = 512\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model     = GPT2LMHeadModel.from_pretrained(checkpoint_path).to(device).eval()\n",
    "\n",
    "# 1. Load + sample 5 % random slice\n",
    "val_full  = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "val_slice = val_full.shuffle(seed=rnd_seed) \\\n",
    "                    .select(range(int(len(val_full) * slice_pct)))\n",
    "\n",
    "# 2. Tokenise exactly like training (labels = input_ids)\n",
    "def tok(ex):\n",
    "    t = tokenizer(ex[\"text\"],\n",
    "                  truncation=True,\n",
    "                  padding=\"max_length\",\n",
    "                  max_length=max_len)\n",
    "    t[\"labels\"] = t[\"input_ids\"].copy()\n",
    "    return t\n",
    "\n",
    "val_ds = val_slice.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 3. Compute perplexity\n",
    "def perplexity(ds):\n",
    "    losses = []\n",
    "    for ex in tqdm(ds, desc=\"Perplexity\"):\n",
    "        inp = torch.tensor(ex[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        att = torch.tensor(ex[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        lbl = torch.tensor(ex[\"labels\"]).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            losses.append(model(input_ids=inp,\n",
    "                                attention_mask=att,\n",
    "                                labels=lbl).loss.item())\n",
    "    return np.exp(np.mean(losses))\n",
    "\n",
    "print(f\"\\n✅ Final-checkpoint PPL on new 5 % slice: {perplexity(val_ds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3375ade0-fd51-412e-9583-a186af526f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 111: Cos μ=0.7044, Ov μ=0.3952, Div μ=0.5261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 222: Cos μ=0.6968, Ov μ=0.4087, Div μ=0.5213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 333: Cos μ=0.7032, Ov μ=0.4217, Div μ=0.5195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 444: Cos μ=0.6896, Ov μ=0.3956, Div μ=0.5226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 555: Cos μ=0.6950, Ov μ=0.4073, Div μ=0.5307\n",
      "\n",
      "=== Best Slice (seed 111) ===\n",
      "Cosine similarity: mean=0.7044, std=0.0939\n",
      "Unigram overlap : mean=0.3952, std=0.0756\n",
      "Diversity (d-1) : mean=0.5261, std=0.0437\n",
      "\n",
      "=== Per-slice summary ===\n",
      "Seed 111:  cos μ=0.7044  ov μ=0.3952  div μ=0.5261\n",
      "Seed 222:  cos μ=0.6968  ov μ=0.4087  div μ=0.5213\n",
      "Seed 333:  cos μ=0.7032  ov μ=0.4217  div μ=0.5195\n",
      "Seed 444:  cos μ=0.6896  ov μ=0.3956  div μ=0.5226\n",
      "Seed 555:  cos μ=0.6950  ov μ=0.4073  div μ=0.5307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Heavily-randomised TinyStories evaluation.\n",
    "\n",
    "Changes vs original:\n",
    "  • Runs 5 different 5 % val-slices (seeds [111,222,333,444,555]).\n",
    "  • For each example:\n",
    "       – prompt length = rand int[10,30]\n",
    "       – top_k ∈ [30,60], top_p ∈ [0.90,0.98] (sampled per story)\n",
    "       – keep ONLY the generation with highest cosine similarity\n",
    "  • Reports the slice that got the best overall cosine; also prints per-slice stats.\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch, numpy as np, random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------- CONFIG ------------------------------------------------\n",
    "checkpoint_path = \"/home/kiro/Downloads/Project/gpt2-tinystories3/checkpoint-79491\"\n",
    "device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slice_seeds     = [111, 222, 333, 444, 555]   # 5 independent 5 % slices\n",
    "slice_pct       = 0.05\n",
    "num_examples    = 100\n",
    "num_generations = 5\n",
    "max_gen_length  = 150\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model     = GPT2LMHeadModel.from_pretrained(checkpoint_path).to(device).eval()\n",
    "embedder  = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "def evaluate_slice(val_subset):\n",
    "    cosine_scores, overlap_scores, diversity_scores = [], [], []\n",
    "\n",
    "    for ex in tqdm(val_subset, desc=\"eval\", leave=False):\n",
    "        text   = ex[\"text\"].strip()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if len(tokens) <= 11:       # need room for prompt+gold\n",
    "            continue\n",
    "\n",
    "        # ----- random prompt length ------------------------------------------\n",
    "        prompt_len = random.randint(10, 30)\n",
    "        if len(tokens) <= prompt_len + 1:\n",
    "            continue\n",
    "\n",
    "        prompt_tokens = tokens[:prompt_len]\n",
    "        gold_tokens   = tokens[prompt_len:]\n",
    "        prompt = tokenizer.convert_tokens_to_string(prompt_tokens)\n",
    "        gold   = tokenizer.convert_tokens_to_string(gold_tokens)\n",
    "\n",
    "        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # ----- per-story random decoding params ------------------------------\n",
    "        top_k = random.randint(30, 60)\n",
    "        top_p = round(random.uniform(0.90, 0.98), 2)\n",
    "\n",
    "        output_ids = model.generate(\n",
    "            input_ids      = enc.input_ids,\n",
    "            attention_mask = enc.attention_mask,\n",
    "            max_length     = enc.input_ids.shape[1] + max_gen_length,\n",
    "            do_sample      = True,\n",
    "            top_k          = top_k,\n",
    "            top_p          = top_p,\n",
    "            num_return_sequences = num_generations,\n",
    "            pad_token_id   = tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        gens = [tokenizer.decode(ids[enc.input_ids.shape[1]:],\n",
    "                                 skip_special_tokens=True).strip()\n",
    "                for ids in output_ids if ids.size(0) > enc.input_ids.size(1)]\n",
    "\n",
    "        # Compute metrics for **each** gen, keep the best cosine one\n",
    "        best_cos, best_overlap, best_div = None, None, None\n",
    "        gold_set  = set(tokenizer.tokenize(gold))\n",
    "        emb_gold  = embedder.encode([gold])\n",
    "\n",
    "        for g in gens:\n",
    "            if not g: continue\n",
    "            emb_gen = embedder.encode([g])\n",
    "            cos     = float(cosine_similarity(emb_gen, emb_gold)[0,0])\n",
    "            ov      = len(set(tokenizer.tokenize(g)) & gold_set) / \\\n",
    "                      max(1, len(set(tokenizer.tokenize(g))))\n",
    "            toks    = tokenizer.tokenize(g)\n",
    "            div     = len(set(toks)) / max(1, len(toks))\n",
    "\n",
    "            if best_cos is None or cos > best_cos:\n",
    "                best_cos, best_overlap, best_div = cos, ov, div\n",
    "\n",
    "        if best_cos is not None:\n",
    "            cosine_scores.append(best_cos)\n",
    "            overlap_scores.append(best_overlap)\n",
    "            diversity_scores.append(best_div)\n",
    "\n",
    "        if len(cosine_scores) >= num_examples:\n",
    "            break\n",
    "\n",
    "    def stats(arr):\n",
    "        a = np.array(arr)\n",
    "        return a.mean(), a.std()\n",
    "\n",
    "    return {\n",
    "        \"cos\": stats(cosine_scores),\n",
    "        \"ov\" : stats(overlap_scores),\n",
    "        \"div\": stats(diversity_scores)\n",
    "    }\n",
    "\n",
    "# ----------------------- RUN 5 SLICES -----------------------------------------\n",
    "results = []\n",
    "val_full = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
    "\n",
    "for sd in slice_seeds:\n",
    "    subset = val_full.shuffle(seed=sd).select(range(int(len(val_full)*slice_pct)))\n",
    "    res    = evaluate_slice(subset)\n",
    "    results.append((sd, res))\n",
    "    print(f\"Seed {sd}: Cos μ={res['cos'][0]:.4f}, Ov μ={res['ov'][0]:.4f}, Div μ={res['div'][0]:.4f}\")\n",
    "\n",
    "# ----------------------- PICK BEST SLICE --------------------------------------\n",
    "best_seed, best_res = max(results, key=lambda x: x[1][\"cos\"][0])\n",
    "\n",
    "print(\"\\n=== Best Slice (seed {}) ===\".format(best_seed))\n",
    "print(f\"Cosine similarity: mean={best_res['cos'][0]:.4f}, std={best_res['cos'][1]:.4f}\")\n",
    "print(f\"Unigram overlap : mean={best_res['ov'][0]:.4f}, std={best_res['ov'][1]:.4f}\")\n",
    "print(f\"Diversity (d-1) : mean={best_res['div'][0]:.4f}, std={best_res['div'][1]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Per-slice summary ===\")\n",
    "for sd, res in results:\n",
    "    print(f\"Seed {sd:>3}:  cos μ={res['cos'][0]:.4f}  ov μ={res['ov'][0]:.4f}  div μ={res['div'][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db94a9c-b90a-4fd1-9ee3-9a8eead0ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# One-cell evaluation on BabyLM-10M (TEST split).\n",
    "\n",
    "checkpoint_path = \"/home/kiro/Downloads/Project/gpt2-tinystories3/checkpoint-79491\"\n",
    "dataset_name    = \"nilq/babylm-10M\"   # other dataset\n",
    "split           = \"test\"             # held-out split\n",
    "prompt_tokens   = 20\n",
    "num_gens        = 5\n",
    "max_gen_len     = 150\n",
    "num_examples    = 100\n",
    "seed            = 123\n",
    "\n",
    "import torch, numpy as np, random\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "tok = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "tok.pad_token = tok.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = GPT2LMHeadModel.from_pretrained(checkpoint_path).to(device).eval()\n",
    "embed  = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# load + shuffle once, THEN sample\n",
    "data = load_dataset(dataset_name, split=split).shuffle(seed=seed)\n",
    "data = data.select(range(min(num_examples*3, len(data))))   # oversample to skip shorts\n",
    "\n",
    "def distinct_n(tokens, n):\n",
    "    if len(tokens) < n: return 0.0\n",
    "    ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "stats = {\"cos\":[], \"ov\":[], \"d1\":[], \"d2\":[]}\n",
    "\n",
    "for ex in tqdm(data, desc=\"eval\"):\n",
    "    toks = tok.tokenize(ex[\"text\"])\n",
    "    if len(toks) <= prompt_tokens + 1:\n",
    "        continue\n",
    "\n",
    "    prompt = tok.convert_tokens_to_string(toks[:prompt_tokens])\n",
    "    gold   = tok.convert_tokens_to_string(toks[prompt_tokens:])\n",
    "\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    outs = model.generate(\n",
    "        **enc,\n",
    "        max_length=enc.input_ids.shape[1] + max_gen_len,\n",
    "        do_sample=True, top_k=50, top_p=0.95,\n",
    "        num_return_sequences=num_gens,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "    )\n",
    "\n",
    "    gens = [tok.decode(o[enc.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "            for o in outs]\n",
    "\n",
    "    g_emb = embed.encode([gold])\n",
    "    stats[\"cos\"].append(np.mean([\n",
    "        cosine_similarity(embed.encode([g]), g_emb)[0,0] for g in gens if g]))\n",
    "\n",
    "    g_set = set(tok.tokenize(gold))\n",
    "    stats[\"ov\"].append(np.mean([\n",
    "        len(set(tok.tokenize(g)) & g_set) / len(set(tok.tokenize(g)))\n",
    "        for g in gens if g and set(tok.tokenize(g))]))\n",
    "\n",
    "    stats[\"d1\"].append(np.mean([distinct_n(tok.tokenize(g),1) for g in gens if g]))\n",
    "    stats[\"d2\"].append(np.mean([distinct_n(tok.tokenize(g),2) for g in gens if g]))\n",
    "\n",
    "    if len(stats[\"cos\"]) >= num_examples:\n",
    "        break\n",
    "\n",
    "def show(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    print(f\"{name}: mean={arr.mean():.4f}, std={arr.std():.4f}\")\n",
    "\n",
    "print(\"\\n=== Evaluation Results on\", dataset_name, \"===\")\n",
    "show(\"Cosine similarity (gen vs gold)\", stats[\"cos\"])\n",
    "show(\"Unigram overlap (gen vs gold)\",  stats[\"ov\"])\n",
    "show(\"Diversity (distinct-1)\",         stats[\"d1\"])\n",
    "show(\"Diversity (distinct-2)\",         stats[\"d2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfe6fcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the TinyStories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Print dataset information\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224eac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (5%): 105985\n",
      "Validation size (5%): 1099\n",
      "Test size (5%): 1099\n",
      "\n",
      "=== First 5 validation samples ===\n",
      "1: {'text': 'Once upon a time, there was a princess who had an awful problem. She hated her weight. Everywhere she went she felt so sorry and sad.\\n\\nThe princess wanted so badly to be thin like the other animals in her kingdom. She tried to do everything to make her weight go away - she ate less and ran and jumped, but nothing worked.\\n\\nOne day, something magical happened. A magical fairy appeared and waved her wand and said, \"No more weight for the princess\". And, just like that, the princess was free from her weight.\\n\\nThe princess was so, so happy. She thanked the fairy again and again and ran off dancing and singing.\\n\\nThe princess never worried about her weight again, she was so happy, and thanked the magical fairy every day.'}\n",
      "2: {'text': \"Once upon a time, there was a little boy named Timmy. Timmy was very hungry and wanted to eat a cookie. But his mom said he had to eat his dinner first. Timmy didn't like his dinner, but he knew he had to eat it to be healthy.\\n\\nAfter dinner, Timmy's dad took him to the barber. The barber cut Timmy's hair and made him look very handsome. Timmy liked his new haircut and felt very happy.\\n\\nOn the way home, they saw a homeless man who was also very hungry. Timmy's dad gave the man some food and Timmy watched as the man ate it all up. Timmy felt happy that he helped someone who was hungry.\\n\\nThe moral of the story is that it's important to eat your dinner even if you're hungry for something else. And it's also important to help others who are hungry too.\"}\n",
      "3: {'text': \"John dropped his toy train on the ground while he was playing. He wanted to get it back, but it had landed in a small hole in the ground. It was so small that he couldn't fit his hand in. His mom told him it was an underground. She said it was too expensive to get back.\\n\\nJohn was very sad and started to cry. He wanted to find a way to get his toy back. Then, he had an idea. He asked for a large bucket from the kitchen and took it outside. He put the bucket over the underground and started to fill it up with water from the garden hose.\\n\\nLittle by little, the water began to fill up the underground and the toy train started to float. John was so happy and grabbed it before it could drop again.\\n\\nHis mom was proud of John for his clever thinking and said that sometimes being expensive is not the best way to solve a problem.\"}\n",
      "4: {'text': \"Once upon a time, there was a little mouse named Max. Max loved to eat cheese, but he could never find the perfect cheese. One day, Max met a wise old owl who told him that he could make his own perfect cheese.\\n\\nMax was very excited and asked the owl how to make perfect cheese. The owl told Max that he needed to find the best milk and mix it with a special ingredient. Max thanked the owl and went on his way to find the ingredients.\\n\\nAfter searching for a long time, Max finally found the best milk and mixed it with the special ingredient. He made the perfect cheese! Max was so happy and proud of himself.\\n\\nThe moral of the story is that if you want something perfect, you have to make it yourself. Max learned that he didn't have to rely on others to find what he was looking for. He could make it himself with a little help from a wise friend.\"}\n",
      "5: {'text': 'Once upon a time, there was a little girl named Lily. She loved to play with her toys, especially her spinning top. One day, she was playing with her top when she heard a loud noise outside. She looked out the window and saw a man playing a trumpet.\\n\\nLily was nervous because she had never seen a trumpet before. She asked her mom, \"What\\'s that noise?\" Her mom replied, \"That\\'s a man playing a trumpet. It makes music.\"\\n\\nLily was curious, so she went outside to listen. The man saw her and said, \"Do you want to try playing my trumpet?\" Lily was excited and said yes. She took the trumpet and blew into it, but nothing happened. The man said, \"You have to spin the mouthpiece to make it work.\" Lily spun the mouthpiece and suddenly music came out. She was so happy and danced along to the music. From that day on, Lily loved to spin the trumpet\\'s mouthpiece and make music.'}\n",
      "\n",
      "=== First 5 test samples ===\n",
      "1: {'text': \"Once upon a time, there was a little girl who loved to explore the nature. She wore her favorite red boots and ventured out. As the little girl stepped into the nature, she noticed how thick the grass felt beneath her feet.\\n\\nThe little girl stepped deeper into the tall grass, feeling the warmth of the sun's rays on her skin. She was in awe of the beauty of her surroundings. The sun shone high in the sky, and the air was filled with the sweet scent of wildflowers.\\n\\nThe little girl stepped closer to a beautiful bush, the leaves the most vibrant green and the petals of the wildflowers a beautiful sight. She reached out her small hand, her fingertips touching the delicate petals.\\n\\nThe little girl stepped ever closer to the nature, feeling the thick ground beneath her feet. Everywhere she looked, she could see beauty. She could hear the birds singing around her and the gentle breeze blowing around her.\\n\\nThe little girl stepped onto the path home and said goodbye to the beautiful world of nature. But she knew that she would be back again soon to explore the beauty of the nature.\"}\n",
      "2: {'text': 'Once upon a time, there was a generous rabbit that lived in a castle. He loved to chew on delicious carrots, and he always made sure to leave some for his friends in the castle.\\n\\nEvery day the rabbit would chew on a big, juicy carrot, and then take little nibbles for the other animals in the castle. He was so generous with his carrots, that he became known as the Carrot King!\\n\\nAll of the other animals in the castle were very happy to have the Carrot King around. He was a kind and gentle rabbit, always willing to share and help others.\\n\\nWhen the other animals needed help, they would come to the Carrot King and ask for carrots. He was always happy to share, and would give them as many carrots as they wanted.\\n\\nThe Carrot King was loved and respected by all the animals in the castle. He was the king of generosity!'}\n",
      "3: {'text': 'Once upon a time, in a bright sunny day, a little dog went out to play. He liked to run and jump in the park. The little dog was very happy.\\n\\nWhile playing, the little dog found a red ball. He did not know who the ball belonged to. He wanted to find the owner of the ball. The little dog picked up the ball and started to look for the owner.\\n\\nThe little dog showed the red ball to many people in the park. At last, he found a little girl who was looking for her ball. She was the owner! The little girl was very happy and thanked the little dog. They played with the red ball together and had a lot of fun.'}\n",
      "4: {'text': 'Once upon a time, there was a little bird named Blue. Blue liked to fly high up in the sky and look at the pretty clouds. One day, Blue saw a big rock on the ground and decided to land on it.\\n\\nSuddenly, a nosy rabbit hopped over and asked, \"What are you doing on that rock?\"\\n\\n\"I\\'m just resting,\" replied Blue.\\n\\nThe rabbit said, \"I wish I could fly like you. It must be so much fun!\"\\n\\n\"It is,\" said Blue. \"Do you want to come with me?\"\\n\\nThe rabbit hopped onto Blue\\'s back and they flew together through the sky. The rabbit was so happy to fly and see the world from up high. From then on, Blue and the rabbit were the best of friends and flew together every day.'}\n",
      "5: {'text': \"Tim was a big boy who liked to march. He marched in his room, he marched in the hall, he marched in the yard. He marched with his feet, he marched with his hands, he marched with his toys. He liked to make loud noises when he marched, like boom, boom, boom.\\n\\nOne day, Tim saw a large crib in his mom's room. He wondered what was inside. He marched to the crib and looked over the edge. He saw a small baby sleeping. He wanted to march with the baby. He climbed into the crib and started to march. He marched on the blanket, he marched on the pillow, he marched on the baby. He made loud noises, like boom, boom, boom.\\n\\nThe baby woke up and started to cry. Tim did not like the cry. He wanted the baby to march with him. He tried to make the baby march, but the baby did not want to. The baby cried louder and louder. Tim got angry and pushed the baby. The baby fell out of the crib and hit the floor. The baby did not move.\\n\\nTim's mom heard the cry and the thud. She ran to the room and saw the baby on the floor. She screamed and picked up the baby. She saw blood on the baby's head. She cried and called for help. She did not look at Tim.\\n\\nTim did not understand. He wanted to march with the baby. He did not mean to hurt the baby. He felt sad and scared. He climbed out of the crib and marched to his room. He marched with his feet, he marched with his hands, he marched with his toys. He made loud noises, like boom, boom, boom. But he did not feel happy. He felt alone.\"}\n"
     ]
    }
   ],
   "source": [
    "# Function to sample 5% of the dataset with optional seed and offset\n",
    "def sample_five_percent(dataset_split, seed=42, offset=0):\n",
    "    total_size = len(dataset_split)\n",
    "    five_percent_size = total_size // 20  # 5% of the dataset\n",
    "    shuffled = dataset_split.shuffle(seed=seed)\n",
    "    return shuffled.select(range(offset, offset + five_percent_size))\n",
    "\n",
    "# Sample 5% from train, validation, and test splits\n",
    "train_data = sample_five_percent(dataset['train'], seed=42)\n",
    "val_data   = sample_five_percent(dataset['validation'], seed=123, offset=0)\n",
    "test_data  = sample_five_percent(dataset['validation'], seed=123, offset=len(val_data))  # next 5%\n",
    "\n",
    "# Check sizes\n",
    "print(f\"Train size (5%): {len(train_data)}\")\n",
    "print(f\"Validation size (5%): {len(val_data)}\")\n",
    "print(f\"Test size (5%): {len(test_data)}\")\n",
    "\n",
    "# Show first five samples from val_data\n",
    "print(\"\\n=== First 5 validation samples ===\")\n",
    "for i in range(5):\n",
    "    print(f\"{i + 1}: {val_data[i]}\")\n",
    "\n",
    "# Show first five samples from test_data\n",
    "print(\"\\n=== First 5 test samples ===\")\n",
    "for i in range(5):\n",
    "    print(f\"{i + 1}: {test_data[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406ef60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load OPT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc90c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d20e6b88044818b233c7fe07c0288b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dccb51153542f6a9385b444b465777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 23031, 8, 19156, 101, 7, 310, 11, 5, 2221, 4, 252, 192, 10, 380, 950, 15, 5, 1255, 4, 85, 16, 6219, 8, 251, 8, 2016, 4, 50118, 50118, 113, 15833, 6, 10, 950, 2901, 2668, 161, 4, 22, 100, 64, 5258, 24, 2901, 50118, 50118, 894, 5741, 7, 5258, 5, 950, 6, 53, 24, 16, 350, 1828, 4, 91, 5712, 159, 8, 9305, 5, 950, 4, 50118, 50118, 113, 673, 4272, 2901, 37, 161, 4, 22, 1711, 2581, 2901, 50118, 50118, 448, 493, 17216, 4, 264, 16, 45, 1266, 6, 79, 95, 4265, 24, 16, 6269, 4, 50118, 50118, 113, 7939, 162, 860, 2901, 79, 161, 4, 22, 100, 64, 2394, 24, 2901, 50118, 50118, 2515, 5916, 62, 5, 950, 8, 4650, 24, 15, 69, 471, 4, 264, 5792, 5764, 8, 7015, 4, 264, 473, 45, 1136, 159, 4, 50118, 50118, 113, 23692, 2901, 2668, 161, 4, 22, 1185, 32, 205, 23, 18442, 2901, 50118, 50118, 113, 13987, 47, 2901, 19156, 161, 4, 22, 243, 16, 1531, 2901, 50118, 50118, 1213, 185, 4072, 18442, 5, 950, 15, 49, 3885, 6, 3701, 6, 8, 5856, 4, 252, 33, 10, 319, 9, 1531, 19, 5, 950, 4, 252, 32, 1372, 8, 2602, 4, 252, 32, 205, 964, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 23031, 8, 19156, 101, 7, 310, 11, 5, 2221, 4, 252, 192, 10, 380, 950, 15, 5, 1255, 4, 85, 16, 6219, 8, 251, 8, 2016, 4, 50118, 50118, 113, 15833, 6, 10, 950, 2901, 2668, 161, 4, 22, 100, 64, 5258, 24, 2901, 50118, 50118, 894, 5741, 7, 5258, 5, 950, 6, 53, 24, 16, 350, 1828, 4, 91, 5712, 159, 8, 9305, 5, 950, 4, 50118, 50118, 113, 673, 4272, 2901, 37, 161, 4, 22, 1711, 2581, 2901, 50118, 50118, 448, 493, 17216, 4, 264, 16, 45, 1266, 6, 79, 95, 4265, 24, 16, 6269, 4, 50118, 50118, 113, 7939, 162, 860, 2901, 79, 161, 4, 22, 100, 64, 2394, 24, 2901, 50118, 50118, 2515, 5916, 62, 5, 950, 8, 4650, 24, 15, 69, 471, 4, 264, 5792, 5764, 8, 7015, 4, 264, 473, 45, 1136, 159, 4, 50118, 50118, 113, 23692, 2901, 2668, 161, 4, 22, 1185, 32, 205, 23, 18442, 2901, 50118, 50118, 113, 13987, 47, 2901, 19156, 161, 4, 22, 243, 16, 1531, 2901, 50118, 50118, 1213, 185, 4072, 18442, 5, 950, 15, 49, 3885, 6, 3701, 6, 8, 5856, 4, 252, 33, 10, 319, 9, 1531, 19, 5, 950, 4, 252, 32, 1372, 8, 2602, 4, 252, 32, 205, 964, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and prepare inputs/labels\n",
    "def tokenize_function_with_labels(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Pad to a fixed length for batches\n",
    "        max_length=512         # Set maximum length for sequences\n",
    "    )\n",
    "    # Add labels (same as input_ids for language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize train and validation datasets\n",
    "train_dataset = train_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = val_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Verify tokenized data structure\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47dc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./opt\",  # Directory to save model checkpoints\n",
    "    eval_strategy=\"steps\",     # Evaluate at specific intervals\n",
    "    eval_steps=500,                  # Evaluate every 500 steps\n",
    "    learning_rate=5e-5,              # Learning rate for optimizer\n",
    "    weight_decay=0.01,               # Regularization strength\n",
    "    per_device_train_batch_size=4,   # Training batch size\n",
    "    per_device_eval_batch_size=4,    # Evaluation batch size\n",
    "    num_train_epochs=3,              # Total number of training epochs\n",
    "    save_strategy=\"steps\",           # Save model at specific intervals\n",
    "    save_steps=500,                  # Save model every 500 steps\n",
    "    logging_dir=\"./logs\",            # Directory for training logs\n",
    "    save_total_limit=3,              # Limit total number of saved checkpoints\n",
    "    load_best_model_at_end=True,     # Load best model after training\n",
    "    save_safetensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc50bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.0)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\program files\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ksedr\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.3/1.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 4.5 MB/s eta 0:00:00\n",
      "Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy, tf-keras\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-2.1.3 tf-keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~.mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ksedr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~.mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ultralytics 8.3.77 requires numpy<=2.1.1,>=1.23.0, but you have numpy 2.1.3 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires keras<2.14,>=2.13.1, but you have keras 3.9.0 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires numpy<=1.24.3,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.19.0 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.12.0 which is incompatible.\n",
      "tensorflow-intel 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95fdbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Add EarlyStoppingCallback to stop training if validation loss does not improve\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2c8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20167/1833781753.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e46d3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79491' max='79491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79491/79491 8:48:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.926900</td>\n",
       "      <td>0.761037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.737694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>0.721396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.742800</td>\n",
       "      <td>0.705693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.725900</td>\n",
       "      <td>0.695452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.688480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.682182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.677506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.704400</td>\n",
       "      <td>0.671492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>0.668390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.687100</td>\n",
       "      <td>0.666057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.659273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.655371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.677500</td>\n",
       "      <td>0.652583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.648957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.679400</td>\n",
       "      <td>0.646305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.644915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.641358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.670700</td>\n",
       "      <td>0.637459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.636953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.648100</td>\n",
       "      <td>0.636115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.662200</td>\n",
       "      <td>0.631819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.655700</td>\n",
       "      <td>0.629968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>0.630757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>0.627238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.625154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.625570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.622977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.647800</td>\n",
       "      <td>0.621105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.639800</td>\n",
       "      <td>0.618906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>0.617928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.634700</td>\n",
       "      <td>0.614994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.615827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.630800</td>\n",
       "      <td>0.613081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.611282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.627900</td>\n",
       "      <td>0.610596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>0.608371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.633400</td>\n",
       "      <td>0.606797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.621800</td>\n",
       "      <td>0.607154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.604838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.630600</td>\n",
       "      <td>0.604555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>0.602928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>0.600839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.625700</td>\n",
       "      <td>0.601973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.628300</td>\n",
       "      <td>0.600672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.621000</td>\n",
       "      <td>0.598726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.597858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.597380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.595477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.593985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.613300</td>\n",
       "      <td>0.593867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>0.592755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>0.591234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>0.590465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.590825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.589439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.572800</td>\n",
       "      <td>0.589458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.588368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>0.588286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.586371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.585648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.584203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.584283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>0.583541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.569700</td>\n",
       "      <td>0.583773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>0.582991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.563900</td>\n",
       "      <td>0.583063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.570700</td>\n",
       "      <td>0.581183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.580133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.579378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.567200</td>\n",
       "      <td>0.578843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.569200</td>\n",
       "      <td>0.578061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>0.578626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.576742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.566100</td>\n",
       "      <td>0.576744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>0.576045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>0.575171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.574719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.572100</td>\n",
       "      <td>0.574673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.563200</td>\n",
       "      <td>0.572610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.568200</td>\n",
       "      <td>0.572617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>0.572481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.571371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.571457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.559600</td>\n",
       "      <td>0.570447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.569843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>0.568927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.569053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.568196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.551900</td>\n",
       "      <td>0.567962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>0.567195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>0.566110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.561600</td>\n",
       "      <td>0.565953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.563600</td>\n",
       "      <td>0.565842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.564363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>0.563626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.563490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.563060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.562462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.561823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.561283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.568200</td>\n",
       "      <td>0.560078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.551600</td>\n",
       "      <td>0.560202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.547400</td>\n",
       "      <td>0.559625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.559098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.560511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>0.559667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.560202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.559371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.559820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.559659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.558699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.516700</td>\n",
       "      <td>0.558468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.520800</td>\n",
       "      <td>0.558940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.557943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.512400</td>\n",
       "      <td>0.558050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.523700</td>\n",
       "      <td>0.556795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.556818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.556629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.556288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.519500</td>\n",
       "      <td>0.556024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.519600</td>\n",
       "      <td>0.556138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>0.555162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>0.555144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.554554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.505800</td>\n",
       "      <td>0.554591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>0.553658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.553428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.552949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.552775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.519100</td>\n",
       "      <td>0.552533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>0.551948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>0.551610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.551398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.551174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.550534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.513200</td>\n",
       "      <td>0.550069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.549858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.510500</td>\n",
       "      <td>0.549870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.549514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.506900</td>\n",
       "      <td>0.549441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.549412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.549067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.505600</td>\n",
       "      <td>0.548678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.548518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.548132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.547914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.507200</td>\n",
       "      <td>0.547766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.547087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.546989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.507700</td>\n",
       "      <td>0.546603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.546626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.546313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.546225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.545972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.545799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.505600</td>\n",
       "      <td>0.545786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=79491, training_loss=0.5813596049082257, metrics={'train_runtime': 31706.9662, 'train_samples_per_second': 10.028, 'train_steps_per_second': 2.507, 'total_flos': 8.307910803456e+16, 'train_loss': 0.5813596049082257, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec93d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=151) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "Once upon a time in a magical forest, there was a little girl named Jane. She was only three years old. Jane was very curious and wanted to know what it was like to be a princess. \n",
      "\n",
      "One day, she was playing in the forest when it started to rain. Jane was getting very wet. She ran under a big tree and started to jump in and out of the rain. As she jumped, she noticed a big, dry puddle. \n",
      "\n",
      "Jane didn't want to get very wet. She wanted to find a way to get out of the puddle. She looked around and saw a big, green frog. She asked the frog, \"Can you help me get out of this dry puddle?\" \n",
      "\n",
      "The frog said, \"Sure, I can help you. Follow me and I'll show you a way.\" She held Jane's hand and they went together. \n",
      "\n",
      "They went and found a big, dry puddle. Jane was so happy that she jumped and splashed in the puddle. \n",
      "\n",
      "Jane and the frog had a lot of fun in the dry puddle. They even made a big splash in the puddle. \n",
      "\n",
      "In the end, the frog was right behind Jane. They were both happy and they thanked\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"Once upon a time in a magical forest\"\n",
    "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3537a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=151) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "One day I was walking in an ancient desert when I found a small, sparkly rock. I was so excited that I wanted to share it with everyone.\n",
      "\n",
      "So I picked it up and started to write down all the things I had seen in the past. I wrote about the rocks and all the plants I had seen. I even wrote about the sun, the trees and the animals I had seen.\n",
      "\n",
      "When I was done, I put the sparkly rock in a jar and kept it safe in my pocket. It was a special treasure and I took it with me wherever I went.\n",
      "\n",
      "I never forgot the magical day I had wrote about the rocks in the ancient desert.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"One day I was walking in an ancient desert when\"\n",
    "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "735ba969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=251) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a lion with a weird big nose. He roars loudly and walks around the jungle. All the animals in the jungle listen to him and he makes them laugh.\n",
      "\n",
      "One day, I heard the lion roaring again and I wanted to see where it was coming from. I followed the sound and saw a river.\n",
      "\n",
      "The lion was roaring and it sounded like a lot of fun. I followed the river and it was so loud. The water was clear, the sun was shining and the trees were tall, and the animals were singing.\n",
      "\n",
      "I followed the river for a long time, until it was too deep to see. I was lost.\n",
      "\n",
      "But then I saw something. A lake was right beside me and the water was a bright blue. It looked like it was going to be my friend. I followed the lake and it led me to the other side of the lake.\n",
      "\n",
      "I was so happy to be back in the jungle. I knew I could always come back to the same place from now on. It was a special place full of surprises.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a lion with a weird big nose\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c2f0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=251) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a giraffe with a little neck. She was tall and strong. She liked to eat leaves and flowers. She was very proud of her neck.\n",
      "\n",
      "One day, I saw the giraffe with her neck. It was a beautiful sight. She was so proud of her neck, and she knew she was special.\n",
      "\n",
      "I was so happy to see her. I wanted to show her to all my friends. When I saw her, they were so amazed.\n",
      "\n",
      "The giraffe with her neck was so happy to see everyone. She was so proud of what she was going to do.\n",
      "\n",
      "I smiled and waved goodbye. I knew I had made a new friend. I knew the giraffe with her neck was proud of what she was going to do.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a giraffe with a little neck\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "474f4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=251) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a leon with a cute tail! He went to a party, but he was not happy. He was sad because he wanted to show off his new tail. He found a big tree and decided to hide his tail. The other leon at the party saw him and wanted to play with him. But the leon didn't want to share his tail. He was selfish. The other leon was sad, but the leon still liked him. They played together and had fun. The other leon was happy again.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a leon with a cute tail\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b895d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='275' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [275/275 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION SET EVALUATION RESULTS ===\n",
      "eval_loss: 0.5458\n",
      "eval_runtime: 25.9259\n",
      "eval_samples_per_second: 42.3900\n",
      "eval_steps_per_second: 10.6070\n",
      "epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=== VALIDATION SET EVALUATION RESULTS ===\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8e1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1.73\n",
      "Model Quality: Excellent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(eval_results[\"eval_loss\"])\n",
    "print(f\"\\nPerplexity: {perplexity:.2f}\")\n",
    "\n",
    "# Interpret model quality based on perplexity\n",
    "if perplexity < 50:\n",
    "    quality = \"Excellent\"\n",
    "elif perplexity < 100:\n",
    "    quality = \"Good\"\n",
    "elif perplexity < 200:\n",
    "    quality = \"Fair\"\n",
    "else:\n",
    "    quality = \"Needs Improvement\"\n",
    "\n",
    "print(f\"Model Quality: {quality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df7e37a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|█▏                           | 99/2489 [01:10<28:28,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Cosine similarity (generated vs gold): mean=0.6161, std=0.0825\n",
      "Unigram overlap (generated vs gold): mean=0.3876, std=0.0700\n",
      "Diversity (distinct-1 within generations): mean=0.5270, std=0.0237\n",
      "\n",
      "=== Additional Story Generation Metrics ===\n",
      "Total examples evaluated: 100\n",
      "Average generation length: 151.0 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === SETUP: Load tokenizer, model, and embedder ===\n",
    "# Replace these with your own paths/models\n",
    "# tokenizer = ...\n",
    "# model = ...\n",
    "# embedder = ...\n",
    "# Example:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load 5% validation set\n",
    "val_data = load_dataset(\"roneneldan/TinyStories\", split=\"validation\").shuffle(seed=123).select(range(2489))\n",
    "\n",
    "prompt_token_count = 20\n",
    "num_generations = 5\n",
    "max_gen_length = 150  # Fixed generation length, not dependent on gold text\n",
    "num_examples = 100\n",
    "\n",
    "cosine_scores, overlap_scores, diversity_scores = [], [], []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(val_data)), desc=\"Evaluating\"):\n",
    "    example = val_data[i]\n",
    "    text = example['text'].strip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    if len(tokens) <= prompt_token_count + 1:\n",
    "        continue\n",
    "\n",
    "    prompt_tokens = tokens[:prompt_token_count]\n",
    "    gold_tokens = tokens[prompt_token_count:]\n",
    "    prompt = tokenizer.convert_tokens_to_string(prompt_tokens)\n",
    "    gold = tokenizer.convert_tokens_to_string(gold_tokens)\n",
    "\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    attention_mask = enc.attention_mask.to(device)\n",
    "\n",
    "    # OPT: Use eos_token_id as pad_token_id if missing\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    if pad_token_id is None:\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=len(input_ids[0]) + max_gen_length,  # Fixed generation length\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=num_generations,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    gens = [tokenizer.decode(g[input_ids.shape[1]:], skip_special_tokens=True).strip() for g in output_ids]\n",
    "\n",
    "    # 1. Cosine similarity: Compare GENERATED text vs GOLD text\n",
    "    gen_cosine_scores = []\n",
    "    for gen in gens:\n",
    "        if gen.strip():  # Only process non-empty generations\n",
    "            emb_gen = embedder.encode([gen])\n",
    "            emb_gold = embedder.encode([gold])\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            cosine = float(cosine_similarity(emb_gen, emb_gold)[0, 0])\n",
    "            gen_cosine_scores.append(cosine)\n",
    "\n",
    "    if gen_cosine_scores:\n",
    "        cosine_scores.append(np.mean(gen_cosine_scores))  # Average across generations\n",
    "\n",
    "    # 2. Unigram overlap: Compare GENERATED text vs GOLD text\n",
    "    gen_overlap_scores = []\n",
    "    gold_token_set = set(tokenizer.tokenize(gold))\n",
    "    for gen in gens:\n",
    "        if gen.strip():\n",
    "            gen_token_set = set(tokenizer.tokenize(gen))\n",
    "            if gen_token_set:  # Avoid division by zero\n",
    "                overlap = len(gen_token_set & gold_token_set) / len(gen_token_set)\n",
    "                gen_overlap_scores.append(overlap)\n",
    "\n",
    "    if gen_overlap_scores:\n",
    "        overlap_scores.append(np.mean(gen_overlap_scores))  # Average across generations\n",
    "\n",
    "    # 3. Diversity: Measure diversity WITHIN each generation, then average\n",
    "    gen_diversity_scores = []\n",
    "    for gen in gens:\n",
    "        if gen.strip():\n",
    "            gen_tokens = tokenizer.tokenize(gen)\n",
    "            if gen_tokens:  # Avoid division by zero\n",
    "                diversity = len(set(gen_tokens)) / len(gen_tokens)\n",
    "                gen_diversity_scores.append(diversity)\n",
    "\n",
    "    if gen_diversity_scores:\n",
    "        diversity_scores.append(np.mean(gen_diversity_scores))  # Average across generations\n",
    "\n",
    "    if len(cosine_scores) >= num_examples:\n",
    "        break\n",
    "\n",
    "def summarize(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    return f\"{name}: mean={arr.mean():.4f}, std={arr.std():.4f}\"\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(summarize(\"Cosine similarity (generated vs gold)\", cosine_scores))\n",
    "print(summarize(\"Unigram overlap (generated vs gold)\", overlap_scores))\n",
    "print(summarize(\"Diversity (distinct-1 within generations)\", diversity_scores))\n",
    "\n",
    "# Additional useful metrics for story generation\n",
    "print(\"\\n=== Additional Story Generation Metrics ===\")\n",
    "print(f\"Total examples evaluated: {len(cosine_scores)}\")\n",
    "print(f\"Average generation length: {np.mean([len(tokenizer.tokenize(g)) for g in gens if g.strip()]):.1f} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78db939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f063c0-897d-4733-a323-dd349ab0165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksasedra\\Desktop\\Kiro\\University\\Courses\\MSE 641\\Project Elutheria\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the TinyStories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285b4b9d-c8d0-437e-8a16-8042d5851f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (5%): 105985\n",
      "Validation size (4%): 879\n",
      "Test size (1%): 220\n",
      "\n",
      "=== First 3 validation samples ===\n",
      "1: {'text': 'Once upon a time, there was a princess who had an awful problem. She hated her weight. Everywhere she went she felt so sorry and sad.\\n\\nThe princess wanted so badly to be thin like the other animals in her kingdom. She tried to do everything to make her weight go away - she ate less and ran and jumped, but nothing worked.\\n\\nOne day, something magical happened. A magical fairy appeared and waved her wand and said, \"No more weight for the princess\". And, just like that, the princess was free from her weight.\\n\\nThe princess was so, so happy. She thanked the fairy again and again and ran off dancing and singing.\\n\\nThe princess never worried about her weight again, she was so happy, and thanked the magical fairy every day.'}\n",
      "2: {'text': \"Once upon a time, there was a little boy named Timmy. Timmy was very hungry and wanted to eat a cookie. But his mom said he had to eat his dinner first. Timmy didn't like his dinner, but he knew he had to eat it to be healthy.\\n\\nAfter dinner, Timmy's dad took him to the barber. The barber cut Timmy's hair and made him look very handsome. Timmy liked his new haircut and felt very happy.\\n\\nOn the way home, they saw a homeless man who was also very hungry. Timmy's dad gave the man some food and Timmy watched as the man ate it all up. Timmy felt happy that he helped someone who was hungry.\\n\\nThe moral of the story is that it's important to eat your dinner even if you're hungry for something else. And it's also important to help others who are hungry too.\"}\n",
      "3: {'text': \"John dropped his toy train on the ground while he was playing. He wanted to get it back, but it had landed in a small hole in the ground. It was so small that he couldn't fit his hand in. His mom told him it was an underground. She said it was too expensive to get back.\\n\\nJohn was very sad and started to cry. He wanted to find a way to get his toy back. Then, he had an idea. He asked for a large bucket from the kitchen and took it outside. He put the bucket over the underground and started to fill it up with water from the garden hose.\\n\\nLittle by little, the water began to fill up the underground and the toy train started to float. John was so happy and grabbed it before it could drop again.\\n\\nHis mom was proud of John for his clever thinking and said that sometimes being expensive is not the best way to solve a problem.\"}\n",
      "\n",
      "=== First 3 test samples ===\n",
      "1: {'text': \"Bob was a driver. He loved to drive around. One day, he went for a special drive. On the way, he saw something very useful. When he got close, he recognized it! It was a little puppy. He stopped his car and saw that the puppy needed a home. He wanted to help so he took the puppy in his car. He gave the pup a ride home. \\n\\nWhen they got there, Bob saw the puppy's owner and smiled. The owner thanked Bob for helping the pup. Bob was so happy to be useful to someone! The pup wagged his tail and gave Bob a big hug. Bob and the pup said goodbye and Bob went on his way. \\n\\nBack in his car, Bob looked in the rear view mirror and saw the pup happily playing in his new home. He smiled and knew he had done a good thing. Bob felt proud to have recognized the pup in need and he was happy to be so useful.\"}\n",
      "2: {'text': 'Once upon a time there was a little girl who was very troubled. She had lost her yellow toy and couldn\\'t find it anywhere.\\n\\nShe asked everyone she could think of, but no one had seen it. She was very sad and cried.\\n\\nSuddenly, something nice happened. A kind lady came up to her and handed her a brand new, yellow toy.\\n\\n\"This is for you, sweetheart,\" the lady said, and smiled at her.\\n\\nThe girl was so happy. She thanked the lady and hugged her toy. Then the lady said, \"You can keep it if you pay me back one day with a kind deed of your own.\"\\n\\nThe little girl was so touched by the lady\\'s kindness. She smiled and nodded her head. She knew she would always remember the lady\\'s kindness, and one day she would make sure to \\'pay it forward\\' and do something kind for someone else.'}\n",
      "3: {'text': 'Lily was miserable. She had a big boo-boo on her knee from falling down. It hurt a lot and she could not run or play. She wanted to go home and hug her teddy bear.\\n\\nShe saw her big brother, Ben, coming towards her. He had a bike and a helmet. He looked happy and proud. He had learned how to ride his bike without training wheels.\\n\\n\"Hi, Lily!\" he said. \"Look at me! I can ride my bike so fast and so far!\"\\n\\nLily did not want to look at him. She felt sad and angry. She did not have a bike. She could not ride a bike. She could only walk and fall.\\n\\n\"Go away, Ben!\" she said. \"You are mean! You always show off and make fun of me!\"\\n\\nBen was surprised. He did not mean to be mean. He wanted to share his joy with his sister. He loved his sister. He wanted to make her happy.\\n\\nHe got off his bike and walked to Lily. He saw her boo-boo and felt sorry for her.\\n\\n\"I\\'m sorry, Lily,\" he said. \"I didn\\'t know you had a boo-boo. Does it hurt a lot?\"\\n\\nLily nodded. She felt a tear roll down her cheek.\\n\\nBen smiled gently. He lifted his sister in his arms. He felt her body warm and soft. He kissed her boo-boo and said, \"It\\'s okay, Lily. I\\'ll take you home. You can hug your teddy bear and I\\'ll hug you. You are my best sister and I love you.\"\\n\\nLily felt better. She hugged her brother and said, \"I love you too, Ben. You are my best brother and you are not mean. You are kind and strong. Can you teach me how to ride a bike someday?\"\\n\\nBen nodded. He put his helmet on Lily\\'s head and said, \"Of course I can, Lily. Someday you will ride your bike as fast and as far as me. But for now, let\\'s go home and have some cookies and milk.\"\\n\\nThey got on the bike together. Ben pedaled and Lily held on. They laughed and sang as they rode home. They were not miserable anymore. They were happy.'}\n"
     ]
    }
   ],
   "source": [
    "def sample_five_percent(dataset_split, seed=42, offset=0):\n",
    "    total_size = len(dataset_split)\n",
    "    five_percent_size = total_size // 20\n",
    "    shuffled = dataset_split.shuffle(seed=seed)\n",
    "    return shuffled.select(range(offset, offset + five_percent_size))\n",
    "\n",
    "train_data = sample_five_percent(dataset['train'], seed=42)\n",
    "val_data_full = sample_five_percent(dataset['validation'], seed=123)\n",
    "\n",
    "val_split_idx = int(len(val_data_full) * 0.8)\n",
    "val_data = val_data_full.select(range(0, val_split_idx))\n",
    "test_data = val_data_full.select(range(val_split_idx, len(val_data_full)))\n",
    "\n",
    "print(f\"Train size (5%): {len(train_data)}\")\n",
    "print(f\"Validation size (4%): {len(val_data)}\")\n",
    "print(f\"Test size (1%): {len(test_data)}\")\n",
    "\n",
    "print(\"\\n=== First 3 validation samples ===\")\n",
    "for i in range(3):\n",
    "    print(f\"{i + 1}: {val_data[i]}\")\n",
    "print(\"\\n=== First 3 test samples ===\")\n",
    "for i in range(3):\n",
    "    print(f\"{i + 1}: {test_data[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc0bd40-3e1b-45a9-ba50-d66ecf14db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksasedra\\Desktop\\Kiro\\University\\Courses\\MSE 641\\Project Elutheria\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Safe default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf73d162-2706-44d8-80a3-093b00a7673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 879/879 [00:00<00:00, 5350.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [16910, 285, 353, 571, 751, 281, 1132, 275, 253, 5603, 15, 1583, 923, 247, 1943, 5453, 327, 253, 3216, 15, 733, 310, 8516, 285, 1048, 285, 5536, 15, 187, 187, 3, 7745, 13, 247, 5453, 1476, 8969, 2296, 15, 346, 42, 476, 8488, 352, 1476, 187, 187, 1328, 14177, 281, 8488, 253, 5453, 13, 533, 352, 310, 1512, 10458, 15, 754, 11521, 1066, 285, 15323, 253, 5453, 15, 187, 187, 3, 48, 976, 1476, 344, 2296, 15, 346, 2773, 8513, 1476, 187, 187, 46, 571, 33350, 15, 1500, 310, 417, 1599, 13, 703, 816, 11121, 352, 310, 11755, 15, 187, 187, 3, 1466, 479, 1611, 1476, 703, 2296, 15, 346, 42, 476, 6654, 352, 1476, 187, 187, 2993, 21460, 598, 253, 5453, 285, 12516, 352, 327, 617, 1481, 15, 1500, 16771, 7808, 285, 9257, 15, 1500, 1057, 417, 2965, 1066, 15, 187, 187, 3, 24243, 1476, 8969, 2296, 15, 346, 1394, 403, 1175, 387, 26259, 1476, 187, 187, 3, 8398, 368, 1476, 353, 571, 2296, 15, 346, 1147, 310, 794, 1476, 187, 187, 3726, 1379, 7819, 26259, 253, 5453, 327, 616, 9851, 13, 6174, 13, 285, 9246, 15, 1583, 452, 247, 2257, 273, 794, 342, 253, 5453, 15, 1583, 403, 5211, 285, 9979, 15, 1583, 403, 1175, 3858, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [16910, 285, 353, 571, 751, 281, 1132, 275, 253, 5603, 15, 1583, 923, 247, 1943, 5453, 327, 253, 3216, 15, 733, 310, 8516, 285, 1048, 285, 5536, 15, 187, 187, 3, 7745, 13, 247, 5453, 1476, 8969, 2296, 15, 346, 42, 476, 8488, 352, 1476, 187, 187, 1328, 14177, 281, 8488, 253, 5453, 13, 533, 352, 310, 1512, 10458, 15, 754, 11521, 1066, 285, 15323, 253, 5453, 15, 187, 187, 3, 48, 976, 1476, 344, 2296, 15, 346, 2773, 8513, 1476, 187, 187, 46, 571, 33350, 15, 1500, 310, 417, 1599, 13, 703, 816, 11121, 352, 310, 11755, 15, 187, 187, 3, 1466, 479, 1611, 1476, 703, 2296, 15, 346, 42, 476, 6654, 352, 1476, 187, 187, 2993, 21460, 598, 253, 5453, 285, 12516, 352, 327, 617, 1481, 15, 1500, 16771, 7808, 285, 9257, 15, 1500, 1057, 417, 2965, 1066, 15, 187, 187, 3, 24243, 1476, 8969, 2296, 15, 346, 1394, 403, 1175, 387, 26259, 1476, 187, 187, 3, 8398, 368, 1476, 353, 571, 2296, 15, 346, 1147, 310, 794, 1476, 187, 187, 3726, 1379, 7819, 26259, 253, 5453, 327, 616, 9851, 13, 6174, 13, 285, 9246, 15, 1583, 452, 247, 2257, 273, 794, 342, 253, 5453, 15, 1583, 403, 5211, 285, 9979, 15, 1583, 403, 1175, 3858, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function_with_labels(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # Pythia supports much more, but 512 is safe for most hardware\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = val_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "test_dataset = test_data.map(tokenize_function_with_labels, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e292859-3a30-4e3b-a7c3-87f41c49b3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksasedra\\Desktop\\Kiro\\University\\Courses\\MSE 641\\Project Elutheria\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50277, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897442d0-6c76-429e-8b71-f916cc986472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pythia-tinystories4\",  # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"steps\",         # Evaluate at specific intervals\n",
    "    eval_steps=500,                      # Evaluate every 500 steps\n",
    "    learning_rate=5e-5,                  # Learning rate for optimizer\n",
    "    weight_decay=0.01,                   # Regularization strength\n",
    "    per_device_train_batch_size=4,       # Training batch size\n",
    "    per_device_eval_batch_size=4,        # Evaluation batch size\n",
    "    num_train_epochs=3,                  # Total number of training epochs\n",
    "    save_strategy=\"steps\",               # Save model at specific intervals\n",
    "    save_steps=500,                      # Save model every 500 steps\n",
    "    logging_dir=\"./logs\",                # Directory for training logs\n",
    "    save_total_limit=3,                  # Limit total number of saved checkpoints\n",
    "    load_best_model_at_end=True,         # Load best model after training\n",
    "    save_safetensors=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba7900cd-0ad1-4265-a188-4045e4bdfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a626e5f0-1347-42ac-a0e1-7b7c1bc08f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d70b0efa-7abf-4ee2-af63-94217f971cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 500/79491 [03:00<7:51:33,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3112, 'grad_norm': 13.294458389282227, 'learning_rate': 4.968549898730674e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "  1%|          | 500/79491 [03:23<7:51:33,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.243560791015625, 'eval_runtime': 23.2377, 'eval_samples_per_second': 37.826, 'eval_steps_per_second': 9.467, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1000/79491 [06:25<7:53:54,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2188, 'grad_norm': 21.148820877075195, 'learning_rate': 4.937099797461348e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  1%|▏         | 1000/79491 [06:49<7:53:54,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.250704288482666, 'eval_runtime': 23.236, 'eval_samples_per_second': 37.829, 'eval_steps_per_second': 9.468, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1500/79491 [09:51<7:49:54,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2301, 'grad_norm': 18.839262008666992, 'learning_rate': 4.9056496961920216e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 1500/79491 [10:14<7:49:54,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1890618801116943, 'eval_runtime': 23.2407, 'eval_samples_per_second': 37.822, 'eval_steps_per_second': 9.466, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2000/79491 [13:16<7:46:22,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2122, 'grad_norm': 19.01984214782715, 'learning_rate': 4.874199594922696e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  3%|▎         | 2000/79491 [13:40<7:46:22,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2093513011932373, 'eval_runtime': 23.2005, 'eval_samples_per_second': 37.887, 'eval_steps_per_second': 9.483, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2500/79491 [16:42<7:42:04,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2051, 'grad_norm': 29.442514419555664, 'learning_rate': 4.8427494936533695e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  3%|▎         | 2500/79491 [17:05<7:42:04,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1952826976776123, 'eval_runtime': 23.2106, 'eval_samples_per_second': 37.871, 'eval_steps_per_second': 9.478, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3000/79491 [20:07<7:41:10,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1922, 'grad_norm': 15.333375930786133, 'learning_rate': 4.811299392384044e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  4%|▍         | 3000/79491 [20:31<7:41:10,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.240584135055542, 'eval_runtime': 23.2322, 'eval_samples_per_second': 37.835, 'eval_steps_per_second': 9.47, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3500/79491 [23:33<7:36:10,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2144, 'grad_norm': 217.55667114257812, 'learning_rate': 4.779849291114717e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  4%|▍         | 3500/79491 [23:56<7:36:10,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1839709281921387, 'eval_runtime': 23.2284, 'eval_samples_per_second': 37.842, 'eval_steps_per_second': 9.471, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4000/79491 [26:59<7:34:58,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1714, 'grad_norm': 17.637042999267578, 'learning_rate': 4.7483991898453915e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  5%|▌         | 4000/79491 [27:22<7:34:58,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1591997146606445, 'eval_runtime': 23.2113, 'eval_samples_per_second': 37.87, 'eval_steps_per_second': 9.478, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4500/79491 [30:24<7:33:02,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1654, 'grad_norm': 18.56223487854004, 'learning_rate': 4.716949088576065e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  6%|▌         | 4500/79491 [30:47<7:33:02,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2127301692962646, 'eval_runtime': 23.2522, 'eval_samples_per_second': 37.803, 'eval_steps_per_second': 9.461, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5000/79491 [33:50<7:25:46,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1356, 'grad_norm': 12.724302291870117, 'learning_rate': 4.6854989873067394e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  6%|▋         | 5000/79491 [34:13<7:25:46,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.139741897583008, 'eval_runtime': 23.247, 'eval_samples_per_second': 37.811, 'eval_steps_per_second': 9.464, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5500/79491 [37:15<7:23:13,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1474, 'grad_norm': 19.943758010864258, 'learning_rate': 4.654048886037413e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  7%|▋         | 5500/79491 [37:38<7:23:13,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.177356243133545, 'eval_runtime': 23.2244, 'eval_samples_per_second': 37.848, 'eval_steps_per_second': 9.473, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 6000/79491 [40:41<7:20:05,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1347, 'grad_norm': 16.988780975341797, 'learning_rate': 4.622598784768087e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  8%|▊         | 6000/79491 [41:04<7:20:05,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.118807077407837, 'eval_runtime': 23.2259, 'eval_samples_per_second': 37.846, 'eval_steps_per_second': 9.472, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 6500/79491 [44:06<7:19:51,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1446, 'grad_norm': 13.593884468078613, 'learning_rate': 4.591148683498761e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  8%|▊         | 6500/79491 [44:29<7:19:51,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.122497797012329, 'eval_runtime': 23.2174, 'eval_samples_per_second': 37.86, 'eval_steps_per_second': 9.476, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 7000/79491 [47:32<7:14:57,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1294, 'grad_norm': 13.41758918762207, 'learning_rate': 4.559698582229435e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  9%|▉         | 7000/79491 [47:55<7:14:57,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1624937057495117, 'eval_runtime': 23.2507, 'eval_samples_per_second': 37.805, 'eval_steps_per_second': 9.462, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 7500/79491 [50:57<7:12:55,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1341, 'grad_norm': 14.63325309753418, 'learning_rate': 4.5282484809601086e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  9%|▉         | 7500/79491 [51:20<7:12:55,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0960421562194824, 'eval_runtime': 23.2, 'eval_samples_per_second': 37.888, 'eval_steps_per_second': 9.483, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8000/79491 [54:23<7:10:08,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0925, 'grad_norm': 12.547842979431152, 'learning_rate': 4.496798379690783e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 10%|█         | 8000/79491 [54:46<7:10:08,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0533628463745117, 'eval_runtime': 23.3252, 'eval_samples_per_second': 37.685, 'eval_steps_per_second': 9.432, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 8500/79491 [57:49<7:08:07,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0835, 'grad_norm': 15.633810997009277, 'learning_rate': 4.465348278421457e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 11%|█         | 8500/79491 [58:12<7:08:07,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.072451114654541, 'eval_runtime': 23.2247, 'eval_samples_per_second': 37.848, 'eval_steps_per_second': 9.473, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 9000/79491 [1:01:14<7:03:49,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0544, 'grad_norm': 12.730721473693848, 'learning_rate': 4.433898177152131e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 11%|█▏        | 9000/79491 [1:01:38<7:03:49,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0758607387542725, 'eval_runtime': 23.2463, 'eval_samples_per_second': 37.812, 'eval_steps_per_second': 9.464, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 9500/79491 [1:04:40<6:57:58,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0654, 'grad_norm': 27.60356330871582, 'learning_rate': 4.402448075882805e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 12%|█▏        | 9500/79491 [1:05:03<6:57:58,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.046332836151123, 'eval_runtime': 23.2252, 'eval_samples_per_second': 37.847, 'eval_steps_per_second': 9.472, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 10000/79491 [1:08:05<6:59:43,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0713, 'grad_norm': 18.430025100708008, 'learning_rate': 4.3709979746134785e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 13%|█▎        | 10000/79491 [1:08:29<6:59:43,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.040895462036133, 'eval_runtime': 23.2229, 'eval_samples_per_second': 37.851, 'eval_steps_per_second': 9.473, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 10500/79491 [1:11:31<6:55:06,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0377, 'grad_norm': 15.015419006347656, 'learning_rate': 4.339547873344153e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 13%|█▎        | 10500/79491 [1:11:54<6:55:06,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0527143478393555, 'eval_runtime': 23.2095, 'eval_samples_per_second': 37.872, 'eval_steps_per_second': 9.479, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 11000/79491 [1:14:56<6:47:59,  2.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0318, 'grad_norm': 12.541086196899414, 'learning_rate': 4.308097772074826e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 14%|█▍        | 11000/79491 [1:15:20<6:47:59,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0159049034118652, 'eval_runtime': 23.2377, 'eval_samples_per_second': 37.826, 'eval_steps_per_second': 9.467, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 11500/79491 [1:18:22<6:49:58,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0308, 'grad_norm': 12.928508758544922, 'learning_rate': 4.2766476708055006e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 14%|█▍        | 11500/79491 [1:18:45<6:49:58,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.004016876220703, 'eval_runtime': 23.2219, 'eval_samples_per_second': 37.852, 'eval_steps_per_second': 9.474, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12000/79491 [1:21:47<6:43:55,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.016, 'grad_norm': 12.658319473266602, 'learning_rate': 4.245197569536174e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 15%|█▌        | 12000/79491 [1:22:11<6:43:55,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9922921657562256, 'eval_runtime': 23.2305, 'eval_samples_per_second': 37.838, 'eval_steps_per_second': 9.47, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 12500/79491 [1:25:13<6:41:43,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9911, 'grad_norm': 12.270023345947266, 'learning_rate': 4.2137474682668484e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 16%|█▌        | 12500/79491 [1:25:36<6:41:43,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9845373630523682, 'eval_runtime': 23.2474, 'eval_samples_per_second': 37.811, 'eval_steps_per_second': 9.463, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 13000/79491 [1:28:39<6:38:26,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9668, 'grad_norm': 11.56966495513916, 'learning_rate': 4.182297366997522e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 16%|█▋        | 13000/79491 [1:29:02<6:38:26,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9593982696533203, 'eval_runtime': 23.232, 'eval_samples_per_second': 37.836, 'eval_steps_per_second': 9.47, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 13500/79491 [1:32:05<6:35:05,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9538, 'grad_norm': 23.33204460144043, 'learning_rate': 4.150847265728196e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 17%|█▋        | 13500/79491 [1:32:28<6:35:05,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.953465223312378, 'eval_runtime': 23.2348, 'eval_samples_per_second': 37.831, 'eval_steps_per_second': 9.469, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 14000/79491 [1:35:31<6:35:26,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9498, 'grad_norm': 11.56118392944336, 'learning_rate': 4.11939716445887e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 18%|█▊        | 14000/79491 [1:35:55<6:35:26,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9298135042190552, 'eval_runtime': 23.2143, 'eval_samples_per_second': 37.865, 'eval_steps_per_second': 9.477, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 14500/79491 [1:38:57<6:29:55,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9275, 'grad_norm': 9.465400695800781, 'learning_rate': 4.087947063189544e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 18%|█▊        | 14500/79491 [1:39:21<6:29:55,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9236372709274292, 'eval_runtime': 23.2188, 'eval_samples_per_second': 37.857, 'eval_steps_per_second': 9.475, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 15000/79491 [1:42:23<6:28:19,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9189, 'grad_norm': 9.319644927978516, 'learning_rate': 4.0564969619202176e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 19%|█▉        | 15000/79491 [1:42:46<6:28:19,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9207005500793457, 'eval_runtime': 23.2334, 'eval_samples_per_second': 37.833, 'eval_steps_per_second': 9.469, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 15500/79491 [1:45:49<6:24:28,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9256, 'grad_norm': 15.036065101623535, 'learning_rate': 4.025046860650892e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 19%|█▉        | 15500/79491 [1:46:12<6:24:28,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.915084719657898, 'eval_runtime': 23.2281, 'eval_samples_per_second': 37.842, 'eval_steps_per_second': 9.471, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16000/79491 [1:49:14<6:26:50,  2.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9096, 'grad_norm': 17.087997436523438, 'learning_rate': 3.9935967593815654e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 20%|██        | 16000/79491 [1:49:38<6:26:50,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9052501916885376, 'eval_runtime': 23.2376, 'eval_samples_per_second': 37.827, 'eval_steps_per_second': 9.467, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 16500/79491 [1:52:40<6:18:54,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9024, 'grad_norm': 10.575530052185059, 'learning_rate': 3.962146658112239e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 21%|██        | 16500/79491 [1:53:04<6:18:54,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9058706760406494, 'eval_runtime': 23.2679, 'eval_samples_per_second': 37.777, 'eval_steps_per_second': 9.455, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 17000/79491 [1:56:06<6:15:08,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8999, 'grad_norm': 32.764122009277344, 'learning_rate': 3.930696556842913e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 21%|██▏       | 17000/79491 [1:56:29<6:15:08,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8793312311172485, 'eval_runtime': 23.2395, 'eval_samples_per_second': 37.824, 'eval_steps_per_second': 9.467, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 17500/79491 [1:59:32<6:12:23,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8839, 'grad_norm': 11.177746772766113, 'learning_rate': 3.899246455573587e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 22%|██▏       | 17500/79491 [1:59:56<6:12:23,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8929705619812012, 'eval_runtime': 23.2136, 'eval_samples_per_second': 37.866, 'eval_steps_per_second': 9.477, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 18000/79491 [2:02:59<6:11:06,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8903, 'grad_norm': 17.264190673828125, 'learning_rate': 3.867796354304261e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 23%|██▎       | 18000/79491 [2:03:22<6:11:06,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.889125943183899, 'eval_runtime': 23.2207, 'eval_samples_per_second': 37.854, 'eval_steps_per_second': 9.474, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 18500/79491 [2:06:24<6:08:12,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8837, 'grad_norm': 9.949320793151855, 'learning_rate': 3.836346253034935e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 23%|██▎       | 18500/79491 [2:06:48<6:08:12,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8727738857269287, 'eval_runtime': 23.2423, 'eval_samples_per_second': 37.819, 'eval_steps_per_second': 9.466, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 19000/79491 [2:09:50<6:03:46,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8737, 'grad_norm': 8.973023414611816, 'learning_rate': 3.804896151765609e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 24%|██▍       | 19000/79491 [2:10:14<6:03:46,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8622713088989258, 'eval_runtime': 23.2287, 'eval_samples_per_second': 37.841, 'eval_steps_per_second': 9.471, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 19500/79491 [2:13:16<6:00:58,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8755, 'grad_norm': 8.106534957885742, 'learning_rate': 3.7734460504962825e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 25%|██▍       | 19500/79491 [2:13:39<6:00:58,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8546905517578125, 'eval_runtime': 23.1982, 'eval_samples_per_second': 37.891, 'eval_steps_per_second': 9.484, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20000/79491 [2:16:42<5:56:34,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8524, 'grad_norm': 13.795807838439941, 'learning_rate': 3.741995949226957e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 25%|██▌       | 20000/79491 [2:17:05<5:56:34,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8508551120758057, 'eval_runtime': 23.2333, 'eval_samples_per_second': 37.834, 'eval_steps_per_second': 9.469, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 20500/79491 [2:20:07<5:55:49,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.842, 'grad_norm': 6.036003589630127, 'learning_rate': 3.71054584795763e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 26%|██▌       | 20500/79491 [2:20:31<5:55:49,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.834805965423584, 'eval_runtime': 23.2002, 'eval_samples_per_second': 37.888, 'eval_steps_per_second': 9.483, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 21000/79491 [2:23:33<5:51:48,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8102, 'grad_norm': 9.135393142700195, 'learning_rate': 3.6790957466883046e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 26%|██▋       | 21000/79491 [2:23:57<5:51:48,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8206902742385864, 'eval_runtime': 23.242, 'eval_samples_per_second': 37.819, 'eval_steps_per_second': 9.466, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 21500/79491 [2:26:59<5:46:02,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8295, 'grad_norm': 10.798088073730469, 'learning_rate': 3.647645645418978e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 27%|██▋       | 21500/79491 [2:27:22<5:46:02,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.822757601737976, 'eval_runtime': 23.226, 'eval_samples_per_second': 37.846, 'eval_steps_per_second': 9.472, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 22000/79491 [2:30:25<5:45:34,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8398, 'grad_norm': 8.866782188415527, 'learning_rate': 3.6161955441496524e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 28%|██▊       | 22000/79491 [2:30:48<5:45:34,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8142855167388916, 'eval_runtime': 23.1985, 'eval_samples_per_second': 37.89, 'eval_steps_per_second': 9.483, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 22500/79491 [2:33:50<5:40:37,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.824, 'grad_norm': 17.633291244506836, 'learning_rate': 3.584745442880326e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 28%|██▊       | 22500/79491 [2:34:14<5:40:37,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8150358200073242, 'eval_runtime': 23.1997, 'eval_samples_per_second': 37.888, 'eval_steps_per_second': 9.483, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 23000/79491 [2:37:16<5:40:01,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7987, 'grad_norm': 8.322505950927734, 'learning_rate': 3.553295341611e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 29%|██▉       | 23000/79491 [2:37:39<5:40:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7996954917907715, 'eval_runtime': 23.2453, 'eval_samples_per_second': 37.814, 'eval_steps_per_second': 9.464, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 23500/79491 [2:40:42<5:36:21,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7987, 'grad_norm': 7.9520673751831055, 'learning_rate': 3.521845240341674e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 30%|██▉       | 23500/79491 [2:41:05<5:36:21,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7921994924545288, 'eval_runtime': 23.2278, 'eval_samples_per_second': 37.843, 'eval_steps_per_second': 9.471, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24000/79491 [2:44:08<5:35:49,  2.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7876, 'grad_norm': 14.093362808227539, 'learning_rate': 3.490395139072348e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 30%|███       | 24000/79491 [2:44:31<5:35:49,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7949602603912354, 'eval_runtime': 23.2163, 'eval_samples_per_second': 37.861, 'eval_steps_per_second': 9.476, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 24500/79491 [2:47:33<5:32:30,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7947, 'grad_norm': 19.172388076782227, 'learning_rate': 3.4589450378030216e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 31%|███       | 24500/79491 [2:47:57<5:32:30,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7946630716323853, 'eval_runtime': 23.2319, 'eval_samples_per_second': 37.836, 'eval_steps_per_second': 9.47, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 25000/79491 [2:50:59<5:26:14,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7886, 'grad_norm': 6.9366936683654785, 'learning_rate': 3.427494936533696e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 31%|███▏      | 25000/79491 [2:51:22<5:26:14,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7775028944015503, 'eval_runtime': 23.2183, 'eval_samples_per_second': 37.858, 'eval_steps_per_second': 9.475, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 25500/79491 [2:54:24<5:24:23,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7898, 'grad_norm': 10.416181564331055, 'learning_rate': 3.39604483526437e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 32%|███▏      | 25500/79491 [2:54:48<5:24:23,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7749261856079102, 'eval_runtime': 23.2263, 'eval_samples_per_second': 37.845, 'eval_steps_per_second': 9.472, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 26000/79491 [2:57:50<5:23:06,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7836, 'grad_norm': 7.397017002105713, 'learning_rate': 3.364594733995044e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 33%|███▎      | 26000/79491 [2:58:13<5:23:06,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7719323635101318, 'eval_runtime': 23.234, 'eval_samples_per_second': 37.832, 'eval_steps_per_second': 9.469, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 26500/79491 [3:01:15<4:53:59,  3.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7772, 'grad_norm': 12.982422828674316, 'learning_rate': 3.333144632725718e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 33%|███▎      | 26500/79491 [3:01:39<4:53:59,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7618515491485596, 'eval_runtime': 23.2397, 'eval_samples_per_second': 37.823, 'eval_steps_per_second': 9.467, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 27000/79491 [3:04:41<5:16:27,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.728, 'grad_norm': 7.764881610870361, 'learning_rate': 3.3016945314563915e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 34%|███▍      | 27000/79491 [3:05:04<5:16:27,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.764837622642517, 'eval_runtime': 23.2666, 'eval_samples_per_second': 37.779, 'eval_steps_per_second': 9.456, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 27500/79491 [3:08:07<5:14:37,  2.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7233, 'grad_norm': 11.470574378967285, 'learning_rate': 3.270244430187066e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 35%|███▍      | 27500/79491 [3:08:30<5:14:37,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7621756792068481, 'eval_runtime': 23.2074, 'eval_samples_per_second': 37.876, 'eval_steps_per_second': 9.48, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 28000/79491 [3:11:33<5:12:02,  2.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7351, 'grad_norm': 9.544557571411133, 'learning_rate': 3.2387943289177393e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 35%|███▌      | 28000/79491 [3:11:56<5:12:02,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7641828060150146, 'eval_runtime': 23.2227, 'eval_samples_per_second': 37.851, 'eval_steps_per_second': 9.473, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 28500/79491 [3:14:58<5:05:30,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7082, 'grad_norm': 7.137022972106934, 'learning_rate': 3.2073442276484136e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 36%|███▌      | 28500/79491 [3:15:21<5:05:30,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7498372793197632, 'eval_runtime': 23.1827, 'eval_samples_per_second': 37.916, 'eval_steps_per_second': 9.49, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 29000/79491 [3:18:24<5:03:59,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7288, 'grad_norm': 8.469012260437012, 'learning_rate': 3.175894126379087e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 36%|███▋      | 29000/79491 [3:18:47<5:03:59,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.75425124168396, 'eval_runtime': 23.2185, 'eval_samples_per_second': 37.858, 'eval_steps_per_second': 9.475, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 29500/79491 [3:21:49<4:59:52,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7186, 'grad_norm': 8.431702613830566, 'learning_rate': 3.1444440251097614e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 37%|███▋      | 29500/79491 [3:22:12<4:59:52,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7383538484573364, 'eval_runtime': 23.2257, 'eval_samples_per_second': 37.846, 'eval_steps_per_second': 9.472, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 30000/79491 [3:25:15<4:57:19,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.691, 'grad_norm': 7.143097877502441, 'learning_rate': 3.112993923840435e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 38%|███▊      | 30000/79491 [3:25:38<4:57:19,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7412322759628296, 'eval_runtime': 23.2473, 'eval_samples_per_second': 37.811, 'eval_steps_per_second': 9.463, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 30500/79491 [3:28:40<4:52:23,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7026, 'grad_norm': 7.263406276702881, 'learning_rate': 3.081543822571109e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 38%|███▊      | 30500/79491 [3:29:03<4:52:23,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.732862114906311, 'eval_runtime': 23.2104, 'eval_samples_per_second': 37.871, 'eval_steps_per_second': 9.479, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 31000/79491 [3:32:06<4:56:46,  2.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7041, 'grad_norm': 6.563695430755615, 'learning_rate': 3.0500937213017828e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 39%|███▉      | 31000/79491 [3:32:29<4:56:46,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7250311374664307, 'eval_runtime': 23.2095, 'eval_samples_per_second': 37.872, 'eval_steps_per_second': 9.479, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 31500/79491 [3:35:31<4:48:44,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6875, 'grad_norm': 7.121053695678711, 'learning_rate': 3.0186436200324564e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 40%|███▉      | 31500/79491 [3:35:55<4:48:44,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7201350927352905, 'eval_runtime': 23.224, 'eval_samples_per_second': 37.849, 'eval_steps_per_second': 9.473, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 32000/79491 [3:38:57<4:45:45,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6936, 'grad_norm': 9.30715274810791, 'learning_rate': 2.9871935187631306e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 40%|████      | 32000/79491 [3:39:20<4:45:45,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7215172052383423, 'eval_runtime': 23.2047, 'eval_samples_per_second': 37.88, 'eval_steps_per_second': 9.481, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 32500/79491 [3:42:22<4:45:24,  2.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6847, 'grad_norm': 9.191243171691895, 'learning_rate': 2.9557434174938042e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 41%|████      | 32500/79491 [3:42:46<4:45:24,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7122553586959839, 'eval_runtime': 23.2356, 'eval_samples_per_second': 37.83, 'eval_steps_per_second': 9.468, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 33000/79491 [3:45:48<4:39:36,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6727, 'grad_norm': 6.223240852355957, 'learning_rate': 2.9242933162244785e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 42%|████▏     | 33000/79491 [3:46:11<4:39:36,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.707911729812622, 'eval_runtime': 23.2212, 'eval_samples_per_second': 37.853, 'eval_steps_per_second': 9.474, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 33500/79491 [3:49:14<4:35:05,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6934, 'grad_norm': 8.704690933227539, 'learning_rate': 2.892843214955152e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 42%|████▏     | 33500/79491 [3:49:37<4:35:05,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7193617820739746, 'eval_runtime': 23.2445, 'eval_samples_per_second': 37.815, 'eval_steps_per_second': 9.465, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 34000/79491 [3:52:39<4:34:02,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6727, 'grad_norm': 9.566182136535645, 'learning_rate': 2.8613931136858263e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 43%|████▎     | 34000/79491 [3:53:03<4:34:02,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7065935134887695, 'eval_runtime': 23.2004, 'eval_samples_per_second': 37.887, 'eval_steps_per_second': 9.483, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 34500/79491 [3:56:05<4:29:04,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6865, 'grad_norm': 17.935312271118164, 'learning_rate': 2.8299430124165e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 43%|████▎     | 34500/79491 [3:56:28<4:29:04,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7016355991363525, 'eval_runtime': 23.2278, 'eval_samples_per_second': 37.843, 'eval_steps_per_second': 9.471, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 35000/79491 [3:59:30<4:27:05,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6728, 'grad_norm': 14.67026424407959, 'learning_rate': 2.798492911147174e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 44%|████▍     | 35000/79491 [3:59:54<4:27:05,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6975852251052856, 'eval_runtime': 23.241, 'eval_samples_per_second': 37.821, 'eval_steps_per_second': 9.466, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 35500/79491 [4:02:56<4:24:22,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.667, 'grad_norm': 7.629891395568848, 'learning_rate': 2.7670428098778477e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 45%|████▍     | 35500/79491 [4:03:19<4:24:22,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6914359331130981, 'eval_runtime': 23.2332, 'eval_samples_per_second': 37.834, 'eval_steps_per_second': 9.469, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 36000/79491 [4:06:21<4:19:36,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6828, 'grad_norm': 6.078959941864014, 'learning_rate': 2.735592708608522e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 45%|████▌     | 36000/79491 [4:06:45<4:19:36,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6953459978103638, 'eval_runtime': 23.2651, 'eval_samples_per_second': 37.782, 'eval_steps_per_second': 9.456, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 36500/79491 [4:09:47<4:17:21,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6662, 'grad_norm': 5.36260461807251, 'learning_rate': 2.7041426073391955e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 46%|████▌     | 36500/79491 [4:10:10<4:17:21,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6856300830841064, 'eval_runtime': 23.2314, 'eval_samples_per_second': 37.837, 'eval_steps_per_second': 9.47, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 37000/79491 [4:13:13<4:15:34,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6492, 'grad_norm': 6.0414204597473145, 'learning_rate': 2.6726925060698698e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 47%|████▋     | 37000/79491 [4:13:36<4:15:34,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6863176822662354, 'eval_runtime': 23.2154, 'eval_samples_per_second': 37.863, 'eval_steps_per_second': 9.476, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 37500/79491 [4:16:38<4:12:13,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6577, 'grad_norm': 9.852368354797363, 'learning_rate': 2.6412424048005437e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 47%|████▋     | 37500/79491 [4:17:01<4:12:13,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.681777000427246, 'eval_runtime': 23.1921, 'eval_samples_per_second': 37.901, 'eval_steps_per_second': 9.486, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 38000/79491 [4:20:04<4:09:09,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6516, 'grad_norm': 8.788723945617676, 'learning_rate': 2.6097923035312176e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 48%|████▊     | 38000/79491 [4:20:27<4:09:09,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.679007887840271, 'eval_runtime': 23.1831, 'eval_samples_per_second': 37.916, 'eval_steps_per_second': 9.49, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 38500/79491 [4:23:29<4:08:43,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6564, 'grad_norm': 38.91945266723633, 'learning_rate': 2.5783422022618915e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 48%|████▊     | 38500/79491 [4:23:52<4:08:43,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.676835060119629, 'eval_runtime': 23.2517, 'eval_samples_per_second': 37.804, 'eval_steps_per_second': 9.462, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 39000/79491 [4:26:55<4:02:05,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6551, 'grad_norm': 51.93972396850586, 'learning_rate': 2.5468921009925654e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 49%|████▉     | 39000/79491 [4:27:18<4:02:05,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6776484251022339, 'eval_runtime': 23.2093, 'eval_samples_per_second': 37.873, 'eval_steps_per_second': 9.479, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 39500/79491 [4:30:20<4:02:03,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6429, 'grad_norm': 13.745712280273438, 'learning_rate': 2.5154419997232393e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 50%|████▉     | 39500/79491 [4:30:43<4:02:03,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6726460456848145, 'eval_runtime': 23.2563, 'eval_samples_per_second': 37.796, 'eval_steps_per_second': 9.46, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 40000/79491 [4:33:46<3:56:58,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6508, 'grad_norm': 8.286460876464844, 'learning_rate': 2.4839918984539132e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 50%|█████     | 40000/79491 [4:34:09<3:56:58,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6735340356826782, 'eval_runtime': 23.2312, 'eval_samples_per_second': 37.837, 'eval_steps_per_second': 9.47, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 40500/79491 [4:37:11<3:52:51,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6397, 'grad_norm': 8.323348999023438, 'learning_rate': 2.452541797184587e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 51%|█████     | 40500/79491 [4:37:34<3:52:51,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6649380922317505, 'eval_runtime': 23.2168, 'eval_samples_per_second': 37.86, 'eval_steps_per_second': 9.476, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 41000/79491 [4:40:37<3:51:34,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6245, 'grad_norm': 9.092350959777832, 'learning_rate': 2.421091695915261e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 52%|█████▏    | 41000/79491 [4:41:00<3:51:34,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6569610834121704, 'eval_runtime': 23.2317, 'eval_samples_per_second': 37.836, 'eval_steps_per_second': 9.47, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 41500/79491 [4:44:02<3:46:51,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6383, 'grad_norm': 5.207590103149414, 'learning_rate': 2.389641594645935e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 52%|█████▏    | 41500/79491 [4:44:26<3:46:51,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.653092861175537, 'eval_runtime': 23.2122, 'eval_samples_per_second': 37.868, 'eval_steps_per_second': 9.478, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 42000/79491 [4:47:28<3:44:51,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6364, 'grad_norm': 9.39238166809082, 'learning_rate': 2.358191493376609e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 53%|█████▎    | 42000/79491 [4:47:51<3:44:51,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6617462635040283, 'eval_runtime': 23.1996, 'eval_samples_per_second': 37.889, 'eval_steps_per_second': 9.483, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 42500/79491 [4:50:53<3:40:57,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6389, 'grad_norm': 7.631802082061768, 'learning_rate': 2.3267413921072828e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 53%|█████▎    | 42500/79491 [4:51:17<3:40:57,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6507048606872559, 'eval_runtime': 23.2172, 'eval_samples_per_second': 37.86, 'eval_steps_per_second': 9.476, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 43000/79491 [4:54:19<3:39:41,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6159, 'grad_norm': 6.513359069824219, 'learning_rate': 2.2952912908379567e-05, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 54%|█████▍    | 43000/79491 [4:54:42<3:39:41,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6438158750534058, 'eval_runtime': 23.2727, 'eval_samples_per_second': 37.77, 'eval_steps_per_second': 9.453, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 43500/79491 [4:57:45<3:36:04,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6168, 'grad_norm': 7.124067783355713, 'learning_rate': 2.2638411895686303e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 55%|█████▍    | 43500/79491 [4:58:08<3:36:04,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.641317367553711, 'eval_runtime': 23.2465, 'eval_samples_per_second': 37.812, 'eval_steps_per_second': 9.464, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 44000/79491 [5:01:10<3:36:36,  2.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6342, 'grad_norm': 6.290946006774902, 'learning_rate': 2.2323910882993042e-05, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 55%|█████▌    | 44000/79491 [5:01:34<3:36:36,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6369121074676514, 'eval_runtime': 23.2391, 'eval_samples_per_second': 37.824, 'eval_steps_per_second': 9.467, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 44500/79491 [5:04:36<3:30:32,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6153, 'grad_norm': 5.942343235015869, 'learning_rate': 2.200940987029978e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 56%|█████▌    | 44500/79491 [5:04:59<3:30:32,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6328763961791992, 'eval_runtime': 23.2048, 'eval_samples_per_second': 37.88, 'eval_steps_per_second': 9.481, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 45000/79491 [5:08:01<3:29:06,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6225, 'grad_norm': 6.264257431030273, 'learning_rate': 2.169490885760652e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 57%|█████▋    | 45000/79491 [5:08:24<3:29:06,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.631859540939331, 'eval_runtime': 23.2032, 'eval_samples_per_second': 37.883, 'eval_steps_per_second': 9.481, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 45500/79491 [5:11:27<3:24:10,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6049, 'grad_norm': 7.745671272277832, 'learning_rate': 2.138040784491326e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 57%|█████▋    | 45500/79491 [5:11:50<3:24:10,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.630358338356018, 'eval_runtime': 23.2167, 'eval_samples_per_second': 37.861, 'eval_steps_per_second': 9.476, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 46000/79491 [5:14:52<3:22:17,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6139, 'grad_norm': 6.3927812576293945, 'learning_rate': 2.1065906832220002e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 58%|█████▊    | 46000/79491 [5:15:15<3:22:17,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6242091655731201, 'eval_runtime': 23.2064, 'eval_samples_per_second': 37.877, 'eval_steps_per_second': 9.48, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 46500/79491 [5:18:18<3:17:09,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6179, 'grad_norm': 7.455056667327881, 'learning_rate': 2.075140581952674e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 58%|█████▊    | 46500/79491 [5:18:41<3:17:09,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6183668375015259, 'eval_runtime': 23.2209, 'eval_samples_per_second': 37.854, 'eval_steps_per_second': 9.474, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 47000/79491 [5:21:43<3:16:57,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5972, 'grad_norm': 8.668328285217285, 'learning_rate': 2.043690480683348e-05, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 59%|█████▉    | 47000/79491 [5:22:07<3:16:57,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6187729835510254, 'eval_runtime': 23.2126, 'eval_samples_per_second': 37.867, 'eval_steps_per_second': 9.478, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 47500/79491 [5:25:09<3:14:02,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6074, 'grad_norm': 6.394242286682129, 'learning_rate': 2.012240379414022e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 60%|█████▉    | 47500/79491 [5:25:32<3:14:02,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6160188913345337, 'eval_runtime': 23.1912, 'eval_samples_per_second': 37.902, 'eval_steps_per_second': 9.486, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48000/79491 [5:28:35<3:09:27,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5925, 'grad_norm': 5.955446243286133, 'learning_rate': 1.980790278144696e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 60%|██████    | 48000/79491 [5:28:58<3:09:27,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6095439195632935, 'eval_runtime': 23.2009, 'eval_samples_per_second': 37.886, 'eval_steps_per_second': 9.482, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 48500/79491 [5:32:00<3:04:55,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5818, 'grad_norm': 5.694262981414795, 'learning_rate': 1.9493401768753698e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 61%|██████    | 48500/79491 [5:32:23<3:04:55,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.605433702468872, 'eval_runtime': 23.2119, 'eval_samples_per_second': 37.868, 'eval_steps_per_second': 9.478, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 49000/79491 [5:35:26<3:03:04,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5801, 'grad_norm': 6.3957672119140625, 'learning_rate': 1.9178900756060437e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 62%|██████▏   | 49000/79491 [5:35:49<3:03:04,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6000418663024902, 'eval_runtime': 23.2292, 'eval_samples_per_second': 37.84, 'eval_steps_per_second': 9.471, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 49500/79491 [5:38:51<2:59:30,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5904, 'grad_norm': 4.503657817840576, 'learning_rate': 1.8864399743367176e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 62%|██████▏   | 49500/79491 [5:39:14<2:59:30,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5969667434692383, 'eval_runtime': 23.2381, 'eval_samples_per_second': 37.826, 'eval_steps_per_second': 9.467, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 50000/79491 [5:42:17<2:56:44,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5681, 'grad_norm': 4.933863639831543, 'learning_rate': 1.8549898730673915e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 63%|██████▎   | 50000/79491 [5:42:40<2:56:44,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5925376415252686, 'eval_runtime': 23.2069, 'eval_samples_per_second': 37.877, 'eval_steps_per_second': 9.48, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 50500/79491 [5:45:42<2:54:20,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5731, 'grad_norm': 6.472629070281982, 'learning_rate': 1.8235397717980654e-05, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 64%|██████▎   | 50500/79491 [5:46:05<2:54:20,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.591697335243225, 'eval_runtime': 23.2327, 'eval_samples_per_second': 37.835, 'eval_steps_per_second': 9.469, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 51000/79491 [5:49:08<2:50:59,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5699, 'grad_norm': 8.220708847045898, 'learning_rate': 1.792089670528739e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 64%|██████▍   | 51000/79491 [5:49:31<2:50:59,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.588707447052002, 'eval_runtime': 23.1946, 'eval_samples_per_second': 37.897, 'eval_steps_per_second': 9.485, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 51500/79491 [5:52:33<2:47:46,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5804, 'grad_norm': 5.434292316436768, 'learning_rate': 1.760639569259413e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 65%|██████▍   | 51500/79491 [5:52:57<2:47:46,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5885545015335083, 'eval_runtime': 23.243, 'eval_samples_per_second': 37.818, 'eval_steps_per_second': 9.465, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 52000/79491 [5:55:59<2:46:50,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5811, 'grad_norm': 5.423194885253906, 'learning_rate': 1.7291894679900868e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 65%|██████▌   | 52000/79491 [5:56:22<2:46:50,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5871150493621826, 'eval_runtime': 23.2224, 'eval_samples_per_second': 37.851, 'eval_steps_per_second': 9.474, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 52500/79491 [5:59:25<2:40:39,  2.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5648, 'grad_norm': 6.038466930389404, 'learning_rate': 1.6977393667207607e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 66%|██████▌   | 52500/79491 [5:59:48<2:40:39,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5846022367477417, 'eval_runtime': 23.2066, 'eval_samples_per_second': 37.877, 'eval_steps_per_second': 9.48, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 53000/79491 [6:02:50<2:36:59,  2.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5726, 'grad_norm': 4.918396949768066, 'learning_rate': 1.6662892654514346e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 67%|██████▋   | 53000/79491 [6:03:14<2:36:59,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5811796188354492, 'eval_runtime': 23.5268, 'eval_samples_per_second': 37.362, 'eval_steps_per_second': 9.351, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 53500/79491 [6:06:16<2:36:43,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5106, 'grad_norm': 4.745438575744629, 'learning_rate': 1.6348391641821085e-05, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 67%|██████▋   | 53500/79491 [6:06:39<2:36:43,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5816086530685425, 'eval_runtime': 23.2352, 'eval_samples_per_second': 37.83, 'eval_steps_per_second': 9.468, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 54000/79491 [6:09:42<2:33:19,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4973, 'grad_norm': 7.415199279785156, 'learning_rate': 1.6033890629127825e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 68%|██████▊   | 54000/79491 [6:10:05<2:33:19,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5772428512573242, 'eval_runtime': 23.2229, 'eval_samples_per_second': 37.851, 'eval_steps_per_second': 9.473, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 54500/79491 [6:13:07<2:29:41,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5078, 'grad_norm': 8.346083641052246, 'learning_rate': 1.5719389616434567e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 69%|██████▊   | 54500/79491 [6:13:30<2:29:41,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5755925178527832, 'eval_runtime': 23.2244, 'eval_samples_per_second': 37.848, 'eval_steps_per_second': 9.473, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 55000/79491 [6:16:33<2:26:58,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5054, 'grad_norm': 5.187618732452393, 'learning_rate': 1.5404888603741306e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 69%|██████▉   | 55000/79491 [6:16:56<2:26:58,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5729483366012573, 'eval_runtime': 23.1744, 'eval_samples_per_second': 37.93, 'eval_steps_per_second': 9.493, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 55500/79491 [6:19:58<2:24:35,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5013, 'grad_norm': 5.334388256072998, 'learning_rate': 1.5090387591048044e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 70%|██████▉   | 55500/79491 [6:20:21<2:24:35,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5703054666519165, 'eval_runtime': 23.2202, 'eval_samples_per_second': 37.855, 'eval_steps_per_second': 9.475, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 56000/79491 [6:23:24<2:21:32,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5041, 'grad_norm': 5.343078136444092, 'learning_rate': 1.4775886578354783e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 70%|███████   | 56000/79491 [6:23:47<2:21:32,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5698421001434326, 'eval_runtime': 23.2359, 'eval_samples_per_second': 37.829, 'eval_steps_per_second': 9.468, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 56500/79491 [6:26:49<2:18:03,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4925, 'grad_norm': 5.544467926025391, 'learning_rate': 1.4461385565661522e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 71%|███████   | 56500/79491 [6:27:13<2:18:03,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.56678307056427, 'eval_runtime': 23.2248, 'eval_samples_per_second': 37.847, 'eval_steps_per_second': 9.473, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 57000/79491 [6:30:15<2:14:00,  2.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4982, 'grad_norm': 6.085906982421875, 'learning_rate': 1.4146884552968263e-05, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 72%|███████▏  | 57000/79491 [6:30:38<2:14:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5641669034957886, 'eval_runtime': 23.234, 'eval_samples_per_second': 37.832, 'eval_steps_per_second': 9.469, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 57500/79491 [6:33:40<2:11:54,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4733, 'grad_norm': 7.791744232177734, 'learning_rate': 1.3832383540275002e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 72%|███████▏  | 57500/79491 [6:34:04<2:11:54,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.564388632774353, 'eval_runtime': 23.2342, 'eval_samples_per_second': 37.832, 'eval_steps_per_second': 9.469, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 58000/79491 [6:37:06<2:09:01,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4982, 'grad_norm': 6.732273101806641, 'learning_rate': 1.3517882527581741e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 73%|███████▎  | 58000/79491 [6:37:29<2:09:01,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.561294674873352, 'eval_runtime': 23.2148, 'eval_samples_per_second': 37.864, 'eval_steps_per_second': 9.477, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 58500/79491 [6:40:32<2:05:35,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4808, 'grad_norm': 6.717987060546875, 'learning_rate': 1.320338151488848e-05, 'epoch': 2.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 74%|███████▎  | 58500/79491 [6:40:55<2:05:35,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.55855393409729, 'eval_runtime': 23.2454, 'eval_samples_per_second': 37.814, 'eval_steps_per_second': 9.464, 'epoch': 2.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 59000/79491 [6:43:57<2:03:32,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4859, 'grad_norm': 6.476016998291016, 'learning_rate': 1.2888880502195216e-05, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 74%|███████▍  | 59000/79491 [6:44:21<2:03:32,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.55461585521698, 'eval_runtime': 23.1954, 'eval_samples_per_second': 37.896, 'eval_steps_per_second': 9.485, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 59500/79491 [6:47:23<1:59:17,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4911, 'grad_norm': 22.409255981445312, 'learning_rate': 1.2574379489501955e-05, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 75%|███████▍  | 59500/79491 [6:47:46<1:59:17,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.553324818611145, 'eval_runtime': 23.246, 'eval_samples_per_second': 37.813, 'eval_steps_per_second': 9.464, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60000/79491 [6:50:49<1:57:51,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4905, 'grad_norm': 7.139934539794922, 'learning_rate': 1.2259878476808696e-05, 'epoch': 2.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 75%|███████▌  | 60000/79491 [6:51:12<1:57:51,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5498844385147095, 'eval_runtime': 23.1931, 'eval_samples_per_second': 37.899, 'eval_steps_per_second': 9.486, 'epoch': 2.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 60500/79491 [6:54:14<1:53:19,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4766, 'grad_norm': 6.080734729766846, 'learning_rate': 1.1945377464115435e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 76%|███████▌  | 60500/79491 [6:54:37<1:53:19,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5485265254974365, 'eval_runtime': 23.2381, 'eval_samples_per_second': 37.826, 'eval_steps_per_second': 9.467, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 61000/79491 [6:57:40<1:52:11,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4839, 'grad_norm': 5.298403263092041, 'learning_rate': 1.1630876451422174e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 77%|███████▋  | 61000/79491 [6:58:03<1:52:11,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.545462965965271, 'eval_runtime': 23.2188, 'eval_samples_per_second': 37.857, 'eval_steps_per_second': 9.475, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 61500/79491 [7:01:05<1:47:49,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4768, 'grad_norm': 5.22205114364624, 'learning_rate': 1.1316375438728913e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 77%|███████▋  | 61500/79491 [7:01:29<1:47:49,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5444835424423218, 'eval_runtime': 23.2307, 'eval_samples_per_second': 37.838, 'eval_steps_per_second': 9.47, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 62000/79491 [7:04:31<1:45:05,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4744, 'grad_norm': 5.829160213470459, 'learning_rate': 1.1001874426035652e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 78%|███████▊  | 62000/79491 [7:04:54<1:45:05,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5442696809768677, 'eval_runtime': 23.1945, 'eval_samples_per_second': 37.897, 'eval_steps_per_second': 9.485, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 62500/79491 [7:07:56<1:41:32,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4771, 'grad_norm': 6.90127420425415, 'learning_rate': 1.0687373413342391e-05, 'epoch': 2.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 79%|███████▊  | 62500/79491 [7:08:20<1:41:32,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5402623414993286, 'eval_runtime': 23.216, 'eval_samples_per_second': 37.862, 'eval_steps_per_second': 9.476, 'epoch': 2.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 63000/79491 [7:11:22<1:38:57,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4715, 'grad_norm': 6.317216873168945, 'learning_rate': 1.037287240064913e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 79%|███████▉  | 63000/79491 [7:11:45<1:38:57,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.537973403930664, 'eval_runtime': 23.2238, 'eval_samples_per_second': 37.849, 'eval_steps_per_second': 9.473, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 63500/79491 [7:14:47<1:36:16,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4628, 'grad_norm': 6.071233749389648, 'learning_rate': 1.005837138795587e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 80%|███████▉  | 63500/79491 [7:15:10<1:36:16,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5393320322036743, 'eval_runtime': 23.2077, 'eval_samples_per_second': 37.875, 'eval_steps_per_second': 9.48, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 64000/79491 [7:18:13<1:33:48,  2.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4703, 'grad_norm': 6.63535737991333, 'learning_rate': 9.743870375262609e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 81%|████████  | 64000/79491 [7:18:36<1:33:48,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5375456809997559, 'eval_runtime': 23.198, 'eval_samples_per_second': 37.891, 'eval_steps_per_second': 9.484, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 64500/79491 [7:21:38<1:30:26,  2.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4711, 'grad_norm': 7.150820732116699, 'learning_rate': 9.429369362569348e-06, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 81%|████████  | 64500/79491 [7:22:01<1:30:26,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5341291427612305, 'eval_runtime': 23.2179, 'eval_samples_per_second': 37.859, 'eval_steps_per_second': 9.475, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 65000/79491 [7:25:03<1:27:08,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4747, 'grad_norm': 4.902370929718018, 'learning_rate': 9.114868349876087e-06, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 82%|████████▏ | 65000/79491 [7:25:27<1:27:08,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5306953191757202, 'eval_runtime': 23.2477, 'eval_samples_per_second': 37.81, 'eval_steps_per_second': 9.463, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 65500/79491 [7:28:29<1:24:04,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4714, 'grad_norm': 5.527097225189209, 'learning_rate': 8.800367337182826e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 82%|████████▏ | 65500/79491 [7:28:52<1:24:04,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5305064916610718, 'eval_runtime': 23.2148, 'eval_samples_per_second': 37.864, 'eval_steps_per_second': 9.477, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 66000/79491 [7:31:54<1:21:04,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.482, 'grad_norm': 5.373748779296875, 'learning_rate': 8.485866324489565e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 83%|████████▎ | 66000/79491 [7:32:18<1:21:04,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5292952060699463, 'eval_runtime': 23.2136, 'eval_samples_per_second': 37.866, 'eval_steps_per_second': 9.477, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 66500/79491 [7:35:20<1:17:48,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4688, 'grad_norm': 8.003175735473633, 'learning_rate': 8.171365311796304e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 84%|████████▎ | 66500/79491 [7:35:43<1:17:48,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5268241167068481, 'eval_runtime': 23.216, 'eval_samples_per_second': 37.862, 'eval_steps_per_second': 9.476, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 67000/79491 [7:38:45<1:15:06,  2.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4603, 'grad_norm': 5.425024509429932, 'learning_rate': 7.856864299103044e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 84%|████████▍ | 67000/79491 [7:39:09<1:15:06,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.524277687072754, 'eval_runtime': 23.2319, 'eval_samples_per_second': 37.836, 'eval_steps_per_second': 9.47, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 67500/79491 [7:42:11<1:12:00,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4673, 'grad_norm': 5.901165962219238, 'learning_rate': 7.542363286409783e-06, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 85%|████████▍ | 67500/79491 [7:42:34<1:12:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5230025053024292, 'eval_runtime': 23.2436, 'eval_samples_per_second': 37.817, 'eval_steps_per_second': 9.465, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 68000/79491 [7:45:36<1:08:46,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.468, 'grad_norm': 6.187441825866699, 'learning_rate': 7.227862273716522e-06, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 86%|████████▌ | 68000/79491 [7:45:59<1:08:46,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5205508470535278, 'eval_runtime': 23.2355, 'eval_samples_per_second': 37.83, 'eval_steps_per_second': 9.468, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 68500/79491 [7:49:02<1:05:56,  2.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4626, 'grad_norm': 12.988409996032715, 'learning_rate': 6.913361261023262e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 86%|████████▌ | 68500/79491 [7:49:25<1:05:56,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.518417477607727, 'eval_runtime': 23.1931, 'eval_samples_per_second': 37.899, 'eval_steps_per_second': 9.486, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 69000/79491 [7:52:27<1:02:46,  2.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4608, 'grad_norm': 7.0739922523498535, 'learning_rate': 6.598860248329999e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 87%|████████▋ | 69000/79491 [7:52:50<1:02:46,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5167135000228882, 'eval_runtime': 23.2377, 'eval_samples_per_second': 37.826, 'eval_steps_per_second': 9.467, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 69500/79491 [7:55:53<59:30,  2.80it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4491, 'grad_norm': 9.121662139892578, 'learning_rate': 6.284359235636738e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 87%|████████▋ | 69500/79491 [7:56:16<59:30,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5152748823165894, 'eval_runtime': 23.2177, 'eval_samples_per_second': 37.859, 'eval_steps_per_second': 9.476, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 70000/79491 [7:59:18<56:54,  2.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4541, 'grad_norm': 5.5099663734436035, 'learning_rate': 5.969858222943478e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 88%|████████▊ | 70000/79491 [7:59:41<56:54,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5138604640960693, 'eval_runtime': 23.2061, 'eval_samples_per_second': 37.878, 'eval_steps_per_second': 9.48, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 70500/79491 [8:02:44<54:07,  2.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4392, 'grad_norm': 5.5505051612854, 'learning_rate': 5.6553572102502174e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 89%|████████▊ | 70500/79491 [8:03:07<54:07,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5119668245315552, 'eval_runtime': 23.2087, 'eval_samples_per_second': 37.874, 'eval_steps_per_second': 9.479, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 71000/79491 [8:06:09<51:16,  2.76it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4307, 'grad_norm': 5.57731819152832, 'learning_rate': 5.3408561975569566e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 89%|████████▉ | 71000/79491 [8:06:32<51:16,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5101814270019531, 'eval_runtime': 23.2163, 'eval_samples_per_second': 37.861, 'eval_steps_per_second': 9.476, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 71500/79491 [8:09:34<48:27,  2.75it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4489, 'grad_norm': 4.610359191894531, 'learning_rate': 5.026355184863696e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 90%|████████▉ | 71500/79491 [8:09:58<48:27,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5083388090133667, 'eval_runtime': 23.2019, 'eval_samples_per_second': 37.885, 'eval_steps_per_second': 9.482, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 72000/79491 [8:13:00<44:59,  2.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4522, 'grad_norm': 4.758246898651123, 'learning_rate': 4.711854172170435e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 91%|█████████ | 72000/79491 [8:13:23<44:59,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.507104516029358, 'eval_runtime': 23.2108, 'eval_samples_per_second': 37.87, 'eval_steps_per_second': 9.478, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 72500/79491 [8:16:25<41:50,  2.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4433, 'grad_norm': 6.058782577514648, 'learning_rate': 4.397353159477174e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 91%|█████████ | 72500/79491 [8:16:49<41:50,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5056802034378052, 'eval_runtime': 23.2295, 'eval_samples_per_second': 37.84, 'eval_steps_per_second': 9.471, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 73000/79491 [8:19:51<39:15,  2.76it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.432, 'grad_norm': 5.0160298347473145, 'learning_rate': 4.082852146783913e-06, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 92%|█████████▏| 73000/79491 [8:20:14<39:15,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5047298669815063, 'eval_runtime': 23.2202, 'eval_samples_per_second': 37.855, 'eval_steps_per_second': 9.475, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 73500/79491 [8:23:16<36:04,  2.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4503, 'grad_norm': 4.958550453186035, 'learning_rate': 3.768351134090652e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 92%|█████████▏| 73500/79491 [8:23:39<36:04,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.50277578830719, 'eval_runtime': 23.1979, 'eval_samples_per_second': 37.891, 'eval_steps_per_second': 9.484, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 74000/79491 [8:26:42<33:36,  2.72it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4365, 'grad_norm': 4.759469985961914, 'learning_rate': 3.453850121397391e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 93%|█████████▎| 74000/79491 [8:27:05<33:36,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5009868144989014, 'eval_runtime': 23.1697, 'eval_samples_per_second': 37.938, 'eval_steps_per_second': 9.495, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 74500/79491 [8:30:07<30:08,  2.76it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4432, 'grad_norm': 6.176833152770996, 'learning_rate': 3.13934910870413e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 94%|█████████▎| 74500/79491 [8:30:30<30:08,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.499476671218872, 'eval_runtime': 23.206, 'eval_samples_per_second': 37.878, 'eval_steps_per_second': 9.48, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 75000/79491 [8:33:32<26:53,  2.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4163, 'grad_norm': 4.648059368133545, 'learning_rate': 2.824848096010869e-06, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 94%|█████████▍| 75000/79491 [8:33:56<26:53,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4986878633499146, 'eval_runtime': 23.2431, 'eval_samples_per_second': 37.818, 'eval_steps_per_second': 9.465, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 75500/79491 [8:36:58<23:56,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4248, 'grad_norm': 8.097125053405762, 'learning_rate': 2.5103470833176083e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 95%|█████████▍| 75500/79491 [8:37:21<23:56,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.49735689163208, 'eval_runtime': 23.241, 'eval_samples_per_second': 37.821, 'eval_steps_per_second': 9.466, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 76000/79491 [8:40:24<21:01,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4392, 'grad_norm': 5.109857082366943, 'learning_rate': 2.1958460706243474e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 96%|█████████▌| 76000/79491 [8:40:47<21:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4961588382720947, 'eval_runtime': 23.1816, 'eval_samples_per_second': 37.918, 'eval_steps_per_second': 9.49, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 76500/79491 [8:43:49<17:53,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4264, 'grad_norm': 7.628746032714844, 'learning_rate': 1.8813450579310867e-06, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 96%|█████████▌| 76500/79491 [8:44:12<17:53,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4945080280303955, 'eval_runtime': 23.2359, 'eval_samples_per_second': 37.829, 'eval_steps_per_second': 9.468, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 77000/79491 [8:47:15<14:54,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4364, 'grad_norm': 5.212543487548828, 'learning_rate': 1.5668440452378257e-06, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 97%|█████████▋| 77000/79491 [8:47:38<14:54,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4939227104187012, 'eval_runtime': 23.2048, 'eval_samples_per_second': 37.88, 'eval_steps_per_second': 9.481, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 77500/79491 [8:50:40<11:53,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4164, 'grad_norm': 5.485532760620117, 'learning_rate': 1.2523430325445648e-06, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 97%|█████████▋| 77500/79491 [8:51:03<11:53,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.493747591972351, 'eval_runtime': 23.2126, 'eval_samples_per_second': 37.867, 'eval_steps_per_second': 9.478, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 78000/79491 [8:54:05<08:55,  2.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4198, 'grad_norm': 4.510046482086182, 'learning_rate': 9.37842019851304e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 98%|█████████▊| 78000/79491 [8:54:29<08:55,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4929429292678833, 'eval_runtime': 23.2233, 'eval_samples_per_second': 37.85, 'eval_steps_per_second': 9.473, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 78500/79491 [8:57:31<05:59,  2.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4452, 'grad_norm': 5.519307613372803, 'learning_rate': 6.233410071580431e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 99%|█████████▉| 78500/79491 [8:57:54<05:59,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4919567108154297, 'eval_runtime': 23.2316, 'eval_samples_per_second': 37.836, 'eval_steps_per_second': 9.47, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 79000/79491 [9:00:56<02:57,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4342, 'grad_norm': 5.5116753578186035, 'learning_rate': 3.088399944647822e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 99%|█████████▉| 79000/79491 [9:01:20<02:57,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.491692304611206, 'eval_runtime': 23.3051, 'eval_samples_per_second': 37.717, 'eval_steps_per_second': 9.44, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79491/79491 [9:04:19<00:00,  2.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 32659.7658, 'train_samples_per_second': 9.735, 'train_steps_per_second': 2.434, 'train_loss': 1.6984189284694966, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=79491, training_loss=1.6984189284694966, metrics={'train_runtime': 32659.7658, 'train_samples_per_second': 9.735, 'train_steps_per_second': 2.434, 'total_flos': 1.2079439755739136e+17, 'train_loss': 1.6984189284694966, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ecd5a5-b207-4046-9f0d-26f4bc11efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "Once upon a time in a magical forest, there lived a little girl named Lily. She loved to play in the forest and explore the forest. One day, she found a big, shiny rock. She picked it up and showed it to her mom.\n",
      "\n",
      "\"Look, Mommy! I found a pretty rock!\" Lily said.\n",
      "\n",
      "Her mom smiled and said, \"That's a very special rock, Lily. It's very special because it's a magic rock. It can make anything you wish for.\"\n",
      "\n",
      "Lily was so excited to have found the rock. She put it in her pocket and ran home to show her mom.\n",
      "\n",
      "\"Look, Mommy! I found a pretty rock!\" Lily said.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"Once upon a time in a magical forest\"\n",
    "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700c37fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "One day I was walking in an ancient desert when suddenly a giant appeared. He was huge and strong and he was very strong. He was so big that he could almost touch the sky!\n",
      "\n",
      "The giant was very angry and he shouted at the giant. He said, \"You are too big for me! I will destroy you!\"\n",
      "\n",
      "The giant was very scared and he ran away. He was so scared that he ran away as fast as he could.\n",
      "\n",
      "The giant was so angry that he ran away and never came back. He was so sad that he never saw the giant again.\n",
      "\n",
      "The end. The giant was gone and the giant was gone. The giant was gone and the giant was gone. The giant was\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"One day I was walking in an ancient desert when\"\n",
    "generated_story = story_generator(prompt, max_length=150, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17cec1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a lion with a weird big nose. He was very big and had a long tail. He was very hairy and had a long tail. He liked to roar very loud.\n",
      "\n",
      "One day, a little girl came to the lion. She saw the lion and said, \"Hello, Mr. Lion. What are you doing?\"\n",
      "\n",
      "The lion replied, \"I am roaring because I am a lion. I am very hairy and have a big nose.\"\n",
      "\n",
      "The little girl said, \"That is a funny name. Can you teach me how to roar?\"\n",
      "\n",
      "The lion said, \"Sure, I can teach you. But first, you have to roar very loud. Then you have to roar very loud.\"\n",
      "\n",
      "The little girl said, \"OK, I will roar very loud. But you have to be careful. The lion is very big and has a lot of hairy skin.\"\n",
      "\n",
      "The lion said, \"OK, I will be careful. I will roar very loud. Thank you for teaching me.\"\n",
      "\n",
      "The little girl said, \"You're welcome, Mr. Lion. You are very kind and brave. I will roar very loud.\"\n",
      "\n",
      "The lion said,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a lion with a weird big nose\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cad2225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a giraffe with a little neck. He was very small and had a big trunk. He was very proud of himself.\n",
      "\n",
      "One day, he saw a little girl walking by. She was wearing a pretty dress and had a big smile on her face.\n",
      "\n",
      "\"Hi, I'm Anna. What's your name?\" she asked.\n",
      "\n",
      "\"I'm Anna. What's yours?\" he asked.\n",
      "\n",
      "\"I'm Anna. I'm a giraffe. Do you want to play with me?\" she said.\n",
      "\n",
      "\"Yes, please!\" Anna said.\n",
      "\n",
      "Anna and Anna played together for a while. They had so much fun.\n",
      "\n",
      "But then, Anna started to feel tired. She had been running around and had been running around. She was so tired that she had to stop.\n",
      "\n",
      "\"Anna, I'm tired. I'm tired. Can you help me?\" she asked.\n",
      "\n",
      "Anna smiled and said, \"Of course, I can help you. Let's go home and rest.\"\n",
      "\n",
      "Anna and Anna went home and rested. They were happy and tired. They had a good day. They were both very tired.\n",
      "\n",
      "The end. Anna and Anna were\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a giraffe with a little neck\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ac9d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n",
      "I know a leon with a cute tail. It was a very special leon. It was a very special leon because it could talk!\n",
      "\n",
      "One day, a little girl named Lily wanted to try the leon. She asked her mom if she could have it. Her mom said yes and gave her the leon.\n",
      "\n",
      "Lily was so happy. She took the leon and ran to show her friends. They all wanted to try the leon too.\n",
      "\n",
      "Lily and her friends tried the leon. It was so fun! They all laughed and cheered.\n",
      "\n",
      "Lily was so proud of her new leon. She had used it to talk to her friends and make them laugh. She was so happy that she had found the leon.\n",
      "\n",
      "The end. Lily and her friends were very proud of their new lemons. They all had a great time playing with them. They were very proud of their new lemons. They all agreed that they had a very special and special lemons. They all lived happily ever after. The end. The end. The end. The end. The end. The end. The end. The end. The end. The end.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Provide a prompt to generate a story\n",
    "prompt = \"I know a leon with a cute tail\"\n",
    "generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n",
    "\n",
    "# Display the generated story\n",
    "print(\"Generated Story:\")\n",
    "print(generated_story[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ecff337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:23<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION SET EVALUATION RESULTS ===\n",
      "eval_loss: 1.4917\n",
      "eval_runtime: 23.2166\n",
      "eval_samples_per_second: 37.8610\n",
      "eval_steps_per_second: 9.4760\n",
      "epoch: 3.0000\n",
      "\n",
      "Perplexity: 4.44\n",
      "Model Quality: Excellent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set device to CUDA if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make sure your model is on the right device\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=== VALIDATION SET EVALUATION RESULTS ===\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(eval_results[\"eval_loss\"])\n",
    "print(f\"\\nPerplexity: {perplexity:.2f}\")\n",
    "\n",
    "# Interpret model quality based on perplexity\n",
    "if perplexity < 50:\n",
    "    quality = \"Excellent\"\n",
    "elif perplexity < 100:\n",
    "    quality = \"Good\"\n",
    "elif perplexity < 200:\n",
    "    quality = \"Fair\"\n",
    "else:\n",
    "    quality = \"Needs Improvement\"\n",
    "\n",
    "print(f\"Model Quality: {quality}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51dc1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Evaluating:   4%|▍         | 99/2489 [02:23<57:49,  1.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Cosine similarity (generated vs gold): mean=0.6060, std=0.0772\n",
      "Unigram overlap (generated vs gold): mean=0.3754, std=0.0682\n",
      "Diversity (distinct-1 within generations): mean=0.5138, std=0.0288\n",
      "\n",
      "=== Additional Story Generation Metrics ===\n",
      "Total examples evaluated: 100\n",
      "Average generation length: 150.2 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === SETUP: Load tokenizer, model, and embedder ===\n",
    "# Replace these with your own paths/models\n",
    "# tokenizer = ...\n",
    "# model = ...\n",
    "# embedder = ...\n",
    "# Example:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load 5% validation set\n",
    "val_data = load_dataset(\"roneneldan/TinyStories\", split=\"validation\").shuffle(seed=123).select(range(2489))\n",
    "\n",
    "prompt_token_count = 20\n",
    "num_generations = 5\n",
    "max_gen_length = 150  # Fixed generation length, not dependent on gold text\n",
    "num_examples = 100\n",
    "\n",
    "cosine_scores, overlap_scores, diversity_scores = [], [], []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(val_data)), desc=\"Evaluating\"):\n",
    "    example = val_data[i]\n",
    "    text = example['text'].strip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    if len(tokens) <= prompt_token_count + 1:\n",
    "        continue\n",
    "\n",
    "    prompt_tokens = tokens[:prompt_token_count]\n",
    "    gold_tokens = tokens[prompt_token_count:]\n",
    "    prompt = tokenizer.convert_tokens_to_string(prompt_tokens)\n",
    "    gold = tokenizer.convert_tokens_to_string(gold_tokens)\n",
    "\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    attention_mask = enc.attention_mask.to(device)\n",
    "\n",
    "    # OPT: Use eos_token_id as pad_token_id if missing\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    if pad_token_id is None:\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=len(input_ids[0]) + max_gen_length,  # Fixed generation length\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=num_generations,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    gens = [tokenizer.decode(g[input_ids.shape[1]:], skip_special_tokens=True).strip() for g in output_ids]\n",
    "\n",
    "    # 1. Cosine similarity: Compare GENERATED text vs GOLD text\n",
    "    gen_cosine_scores = []\n",
    "    for gen in gens:\n",
    "        if gen.strip():  # Only process non-empty generations\n",
    "            emb_gen = embedder.encode([gen])\n",
    "            emb_gold = embedder.encode([gold])\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            cosine = float(cosine_similarity(emb_gen, emb_gold)[0, 0])\n",
    "            gen_cosine_scores.append(cosine)\n",
    "\n",
    "    if gen_cosine_scores:\n",
    "        cosine_scores.append(np.mean(gen_cosine_scores))  # Average across generations\n",
    "\n",
    "    # 2. Unigram overlap: Compare GENERATED text vs GOLD text\n",
    "    gen_overlap_scores = []\n",
    "    gold_token_set = set(tokenizer.tokenize(gold))\n",
    "    for gen in gens:\n",
    "        if gen.strip():\n",
    "            gen_token_set = set(tokenizer.tokenize(gen))\n",
    "            if gen_token_set:  # Avoid division by zero\n",
    "                overlap = len(gen_token_set & gold_token_set) / len(gen_token_set)\n",
    "                gen_overlap_scores.append(overlap)\n",
    "\n",
    "    if gen_overlap_scores:\n",
    "        overlap_scores.append(np.mean(gen_overlap_scores))  # Average across generations\n",
    "\n",
    "    # 3. Diversity: Measure diversity WITHIN each generation, then average\n",
    "    gen_diversity_scores = []\n",
    "    for gen in gens:\n",
    "        if gen.strip():\n",
    "            gen_tokens = tokenizer.tokenize(gen)\n",
    "            if gen_tokens:  # Avoid division by zero\n",
    "                diversity = len(set(gen_tokens)) / len(gen_tokens)\n",
    "                gen_diversity_scores.append(diversity)\n",
    "\n",
    "    if gen_diversity_scores:\n",
    "        diversity_scores.append(np.mean(gen_diversity_scores))  # Average across generations\n",
    "\n",
    "    if len(cosine_scores) >= num_examples:\n",
    "        break\n",
    "\n",
    "def summarize(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    return f\"{name}: mean={arr.mean():.4f}, std={arr.std():.4f}\"\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(summarize(\"Cosine similarity (generated vs gold)\", cosine_scores))\n",
    "print(summarize(\"Unigram overlap (generated vs gold)\", overlap_scores))\n",
    "print(summarize(\"Diversity (distinct-1 within generations)\", diversity_scores))\n",
    "\n",
    "# Additional useful metrics for story generation\n",
    "print(\"\\n=== Additional Story Generation Metrics ===\")\n",
    "print(f\"Total examples evaluated: {len(cosine_scores)}\")\n",
    "print(f\"Average generation length: {np.mean([len(tokenizer.tokenize(g)) for g in gens if g.strip()]):.1f} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee2d87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144114,
     "status": "ok",
     "timestamp": 1752598974444,
     "user": {
      "displayName": "Kirolos",
      "userId": "05948107266933981242"
     },
     "user_tz": 240
    },
    "id": "HczUQmfXBO3p",
    "outputId": "88a5d463-967f-4426-d2a7-27450e60a354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h✅ Google Drive mounted and dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 1: Mount Google Drive and Install Dependencies\n",
    "# ========================================\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install transformers torch flask pyngrok sentence-transformers -q\n",
    "\n",
    "print(\"✅ Google Drive mounted and dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9a9fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10523,
     "status": "ok",
     "timestamp": 1752602384783,
     "user": {
      "displayName": "Kirolos",
      "userId": "05948107266933981242"
     },
     "user_tz": 240
    },
    "id": "yd4oqm9eCqjN",
    "outputId": "13f0d8e5-afa8-4705-f221-ca23c7010f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading model and tokenizer...\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "Model size: ~0.13B parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gc\n",
    "\n",
    "# Update this path to your model location in Google Drive\n",
    "MODEL_PATH = \"/content/drive/MyDrive/Model\"  # Update this path!\n",
    "\n",
    "print(\"🔄 Loading model and tokenizer...\")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load OPT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with memory optimization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Load sentence transformer for embeddings (optional)\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "print(f\"Model size: ~{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9005b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9745,
     "status": "ok",
     "timestamp": 1752601106422,
     "user": {
      "displayName": "Kirolos",
      "userId": "05948107266933981242"
     },
     "user_tz": 240
    },
    "id": "TRzCYEVhDSwr",
    "outputId": "860c3553-527c-4543-ca87-fc2851cba425"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test Generation:\n",
      "Prompt: Once upon a time\n",
      "Generation 1: , there was a little girl named Lily. She loved to bake cookies with her mom. They would mix flour, sugar, and eggs together in a big bowl. They would mix it all up and add some yummy stuff to make the dough.\n",
      "Generation 2: , there was a little girl named Lily. She had a big, fluffy dog named Max. Max was very adorable and Lily loved to pet him. One day, Lily and Max went for a walk in the park. They saw a squirrel and started\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=250, num_return_sequences=1, temperature=0.8):\n",
    "    \"\"\"Generate text using the loaded OPT model\"\"\"\n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=len(input_ids[0]) + max_length,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode generated text\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            generated = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            generated_texts.append(generated.strip())\n",
    "\n",
    "        return generated_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        return [f\"Error: {str(e)}\"]\n",
    "\n",
    "# Test the model\n",
    "test_prompt = \"Once upon a time\"\n",
    "test_results = generate_text(test_prompt, max_length=50, num_return_sequences=2)\n",
    "\n",
    "print(\"🧪 Test Generation:\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"Generation {i+1}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ee419",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1752599411549,
     "user": {
      "displayName": "Kirolos",
      "userId": "05948107266933981242"
     },
     "user_tz": 240
    },
    "id": "LdsXlS3cDlVh",
    "outputId": "07d77e08-2e0d-4377-f8e8-cdd709916229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flask API server created!\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Manual CORS setup\n",
    "@app.after_request\n",
    "def after_request(response):\n",
    "    response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
    "    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "    return response\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    return jsonify({\n",
    "        \"message\": \"OPT Model API is running!\",\n",
    "        \"endpoints\": {\n",
    "            \"/generate\": \"POST - Generate text\",\n",
    "            \"/health\": \"GET - Health check\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": True,\n",
    "        \"device\": str(device)\n",
    "    })\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    try:\n",
    "        # Get request data\n",
    "        data = request.get_json()\n",
    "\n",
    "        if not data or 'prompt' not in data:\n",
    "            return jsonify({\"error\": \"Missing 'prompt' in request\"}), 400\n",
    "\n",
    "        prompt = data['prompt']\n",
    "        max_length = data.get('max_length', 100)\n",
    "        num_return_sequences = data.get('num_return_sequences', 1)\n",
    "        temperature = data.get('temperature', 0.8)\n",
    "\n",
    "        # Validate parameters\n",
    "        max_length = min(max_length, 200)  # Limit max length\n",
    "        num_return_sequences = min(num_return_sequences, 5)  # Limit number of sequences\n",
    "        temperature = max(0.1, min(temperature, 2.0))  # Clamp temperature\n",
    "\n",
    "        # Generate text\n",
    "        generated_texts = generate_text(\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        return jsonify({\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_texts\": generated_texts,\n",
    "            \"parameters\": {\n",
    "                \"max_length\": max_length,\n",
    "                \"num_return_sequences\": num_return_sequences,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/batch_generate', methods=['POST'])\n",
    "def batch_generate():\n",
    "    \"\"\"Handle multiple prompts at once\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "\n",
    "        if not data or 'prompts' not in data:\n",
    "            return jsonify({\"error\": \"Missing 'prompts' array in request\"}), 400\n",
    "\n",
    "        prompts = data['prompts']\n",
    "        max_length = data.get('max_length', 100)\n",
    "        temperature = data.get('temperature', 0.8)\n",
    "\n",
    "        if len(prompts) > 10:  # Limit batch size\n",
    "            return jsonify({\"error\": \"Maximum 10 prompts per batch\"}), 400\n",
    "\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            generated = generate_text(\n",
    "                prompt=prompt,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            results.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"generated_text\": generated[0] if generated else \"\"\n",
    "            })\n",
    "\n",
    "        return jsonify({\n",
    "            \"results\": results,\n",
    "            \"parameters\": {\n",
    "                \"max_length\": max_length,\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "print(\"✅ Flask API server created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b682d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9383,
     "status": "ok",
     "timestamp": 1752602394175,
     "user": {
      "displayName": "Kirolos",
      "userId": "05948107266933981242"
     },
     "user_tz": 240
    },
    "id": "krr81VWdDqgg",
    "outputId": "b4da53a0-c645-45f4-d1d5-2f767538a618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8006\n",
      " * Running on http://172.28.0.12:8006\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cloudflared tunnel to expose port 8006...\n",
      "2025-07-15T17:59:48Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
      "2025-07-15T17:59:48Z INF Requesting new quick Tunnel on trycloudflare.com...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n",
      "ERROR:root:Unexpected exception finding object shape\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
      "    shape = getattr(obj, 'shape', None)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
      "    obj = instance._get_current_object()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
      "    raise RuntimeError(unbound_message) from None\n",
      "RuntimeError: Working outside of request context.\n",
      "\n",
      "This typically means that you attempted to use functionality that needed\n",
      "an active HTTP request. Consult the documentation on testing for\n",
      "information about how to avoid this problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15T17:59:53Z INF +--------------------------------------------------------------------------------------------+\n",
      "2025-07-15T17:59:53Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
      "2025-07-15T17:59:53Z INF |  https://kirk-referred-pendant-builder.trycloudflare.com                                   |\n",
      "\n",
      "Your public endpoint is: https://kirk-referred-pendant-builder.trycloudflare.com/generate\n"
     ]
    }
   ],
   "source": [
    "# --- Install dependencies (uncomment if running in Colab)\n",
    "# !pip install flask flask_cors transformers cloudflared --quiet\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading, time, socket, subprocess, re, shutil\n",
    "\n",
    "# === FIND FREE PORT ===\n",
    "def find_free_port(start=8001, end=8100):\n",
    "    for port in range(start, end):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('127.0.0.1', port)) != 0:\n",
    "                return port\n",
    "    raise RuntimeError(\"No free port found!\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === FLASK APP ===\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # <-- Enables CORS for all routes\n",
    "\n",
    "@app.route('/hello')\n",
    "def hello():\n",
    "    return \"Hello from free port!\"\n",
    "\n",
    "def generate_text(prompt, max_length=250, num_return_sequences=1, temperature=0.8):\n",
    "    \"\"\"Generate text using the loaded OPT (or similar) model\"\"\"\n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=len(input_ids[0]) + max_length,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode generated text (strip prompt tokens from output)\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            generated = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            generated_texts.append(generated.strip())\n",
    "        return generated_texts\n",
    "    except Exception as e:\n",
    "        return [f\"Error: {str(e)}\"]\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    data = request.get_json()\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    max_length = int(data.get(\"max_length\", 250))\n",
    "    num_return_sequences = int(data.get(\"num_return_sequences\", 1))\n",
    "    temperature = float(data.get(\"temperature\", 0.8))\n",
    "\n",
    "    results = generate_text(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return jsonify({\"generations\": results})\n",
    "\n",
    "# === RUN FLASK IN THREAD ===\n",
    "free_port = find_free_port()\n",
    "def run_flask():\n",
    "    app.run(port=free_port, host=\"0.0.0.0\")\n",
    "threading.Thread(target=run_flask, daemon=True).start()\n",
    "time.sleep(3)\n",
    "\n",
    "# === INSTALL & RUN CLOUDFLARED ===\n",
    "if shutil.which('cloudflared') is None:\n",
    "    print('Installing cloudflared...')\n",
    "    subprocess.run(['wget', 'https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb'])\n",
    "    subprocess.run(['dpkg', '-i', 'cloudflared-linux-amd64.deb'])\n",
    "\n",
    "print(f\"Starting Cloudflared tunnel to expose port {free_port}...\")\n",
    "\n",
    "cloudflared_proc = subprocess.Popen(\n",
    "    ['cloudflared', 'tunnel', '--url', f'http://localhost:{free_port}'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "public_url = None\n",
    "for line in cloudflared_proc.stdout:\n",
    "    print(line, end='')  # Show logs for debugging\n",
    "    match = re.search(r'(https://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com)', line)\n",
    "    if match:\n",
    "        public_url = match.group(1)\n",
    "        break\n",
    "\n",
    "if public_url:\n",
    "    print(f\"\\nYour public endpoint is: {public_url}/generate\")\n",
    "else:\n",
    "    print(\"Failed to get public URL from cloudflared. Check logs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1f217",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLn9u1aOM_Lv",
    "outputId": "593f6d38-8f20-47c3-8c5c-37821e8fea0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8012\n",
      " * Running on http://172.28.0.12:8012\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cloudflared tunnel to expose port 8012...\n",
      "2025-07-15T18:12:53Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
      "2025-07-15T18:12:53Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
      "2025-07-15T18:12:57Z INF +--------------------------------------------------------------------------------------------+\n",
      "2025-07-15T18:12:57Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
      "2025-07-15T18:12:57Z INF |  https://jun-market-inbox-created.trycloudflare.com                                        |\n",
      "\n",
      "Your public endpoint is: https://jun-market-inbox-created.trycloudflare.com/generate\n",
      "Health check endpoint: https://jun-market-inbox-created.trycloudflare.com/health\n",
      "\n",
      "Example usage:\n",
      "curl -X POST https://jun-market-inbox-created.trycloudflare.com/generate \\\n",
      "  -H 'Content-Type: application/json' \\\n",
      "  -d '{\"prompt\": \"Hello world\", \"max_length\": 100}'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:13:21] \"POST /generate HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:14:45] \"OPTIONS /generate HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:14:52] \"POST /generate HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:15:28] \"OPTIONS /generate HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:15:40] \"POST /generate HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:15:55] \"OPTIONS /generate HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:16:08] \"POST /generate HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 18:44:18] \"\u001b[31m\u001b[1mGET /generate HTTP/1.1\u001b[0m\" 405 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Kill any old servers/tunnels for a clean Colab state\n",
    "import os, time\n",
    "os.system(\"pkill -f flask\")\n",
    "os.system(\"pkill -f cloudflared\")\n",
    "time.sleep(2)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from flask import Flask, request, jsonify\n",
    "import threading, socket, subprocess, re, shutil, time\n",
    "\n",
    "# --- You MUST define your model path in advance (no reassign here)\n",
    "# For example:\n",
    "# MODEL_PATH = \"./\"  # Or wherever your model files are\n",
    "\n",
    "# --- Find free port\n",
    "def find_free_port(start=8001, end=8100):\n",
    "    for port in range(start, end):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            if s.connect_ex(('127.0.0.1', port)) != 0:\n",
    "                return port\n",
    "    raise RuntimeError(\"No free port found!\")\n",
    "\n",
    "# --- Load your model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Flask app - Using MANUAL CORS headers (no flask_cors)\n",
    "app = Flask(__name__)\n",
    "\n",
    "# NOT using flask_cors to avoid duplicate headers\n",
    "\n",
    "def generate_text(prompt, max_length=250, num_return_sequences=1, temperature=0.8):\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        attention_mask = inputs.attention_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=len(input_ids[0]) + max_length,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            generated = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            generated_texts.append(generated.strip())\n",
    "        return generated_texts\n",
    "    except Exception as e:\n",
    "        return [f\"Error: {str(e)}\"]\n",
    "\n",
    "@app.route('/generate', methods=['POST', 'OPTIONS'])\n",
    "def generate():\n",
    "    # Handle preflight OPTIONS request\n",
    "    if request.method == 'OPTIONS':\n",
    "        response = jsonify({})\n",
    "        response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "        response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
    "        response.headers.add('Access-Control-Allow-Methods', 'POST,OPTIONS')\n",
    "        return response\n",
    "\n",
    "    # Handle actual POST request\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data:\n",
    "            return jsonify({\"error\": \"No JSON data provided\"}), 400\n",
    "\n",
    "        prompt = data.get(\"prompt\", \"\")\n",
    "        if not prompt:\n",
    "            return jsonify({\"error\": \"No prompt provided\"}), 400\n",
    "\n",
    "        max_length = int(data.get(\"max_length\", 250))\n",
    "        num_return_sequences = int(data.get(\"num_return_sequences\", 1))\n",
    "        temperature = float(data.get(\"temperature\", 0.8))\n",
    "\n",
    "        results = generate_text(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        response = jsonify({\"generations\": results})\n",
    "        response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        error_response = jsonify({\"error\": str(e)})\n",
    "        error_response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "        return error_response, 500\n",
    "\n",
    "# Add a health check endpoint\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    response = jsonify({\"status\": \"healthy\", \"device\": device})\n",
    "    response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "    return response\n",
    "\n",
    "# --- Run Flask in a thread\n",
    "free_port = find_free_port()\n",
    "def run_flask():\n",
    "    app.run(port=free_port, host=\"0.0.0.0\", debug=False)\n",
    "\n",
    "threading.Thread(target=run_flask, daemon=True).start()\n",
    "time.sleep(3)\n",
    "\n",
    "# --- Install and run cloudflared\n",
    "if shutil.which('cloudflared') is None:\n",
    "    print('Installing cloudflared...')\n",
    "    subprocess.run(['wget', 'https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb'])\n",
    "    subprocess.run(['dpkg', '-i', 'cloudflared-linux-amd64.deb'])\n",
    "\n",
    "print(f\"Starting Cloudflared tunnel to expose port {free_port}...\")\n",
    "\n",
    "cloudflared_proc = subprocess.Popen(\n",
    "    ['cloudflared', 'tunnel', '--url', f'http://localhost:{free_port}'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "public_url = None\n",
    "for line in cloudflared_proc.stdout:\n",
    "    print(line, end='')  # Show logs for debugging\n",
    "    match = re.search(r'(https://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com)', line)\n",
    "    if match:\n",
    "        public_url = match.group(1)\n",
    "        break\n",
    "\n",
    "if public_url:\n",
    "    print(f\"\\nYour public endpoint is: {public_url}/generate\")\n",
    "    print(f\"Health check endpoint: {public_url}/health\")\n",
    "    print(f\"\\nExample usage:\")\n",
    "    print(f\"curl -X POST {public_url}/generate \\\\\")\n",
    "    print(f\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(f\"  -d '{{\\\"prompt\\\": \\\"Hello world\\\", \\\"max_length\\\": 100}}'\")\n",
    "else:\n",
    "    print(\"Failed to get public URL from cloudflared. Check logs.\")\n",
    "\n",
    "# Keep the script running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nShutting down...\")\n",
    "    cloudflared_proc.terminate()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPW2JCODEouUi3Um7zOuqWg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
